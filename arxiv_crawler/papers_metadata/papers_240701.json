{
    "Physics": [
        {
            "title": "Solving Differential Equations using Physics-Informed Deep Equilibrium Models",
            "authors": "Bruno Machado Pacheco, Eduardo Camponogara",
            "summary": "This paper introduces Physics-Informed Deep Equilibrium Models (PIDEQs) for\nsolving initial value problems (IVPs) of ordinary differential equations\n(ODEs). Leveraging recent advancements in deep equilibrium models (DEQs) and\nphysics-informed neural networks (PINNs), PIDEQs combine the implicit output\nrepresentation of DEQs with physics-informed training techniques. We validate\nPIDEQs using the Van der Pol oscillator as a benchmark problem, demonstrating\ntheir efficiency and effectiveness in solving IVPs. Our analysis includes key\nhyperparameter considerations for optimizing PIDEQ performance. By bridging\ndeep learning and physics-based modeling, this work advances computational\ntechniques for solving IVPs, with implications for scientific computing and\nengineering applications.",
            "pdf_url": "http://arxiv.org/pdf/2406.03472v2",
            "published": "2024-06-05 17:25:29+00:00",
            "updated": "2024-06-28 17:44:28+00:00"
        },
        {
            "title": "Tau Tridents at Accelerator Neutrino Facilities",
            "authors": "Innes Bigaran, P. S. Bhupal Dev, Diego Lopez Gutierrez, Pedro A. N. Machado",
            "summary": "We present the first detailed study of Standard Model (SM) neutrino tridents\ninvolving tau leptons at the near detectors of accelerator neutrino facilities.\nThese processes were previously thought to be negligible, even at future\nfacilities like DUNE, based on approximations that underestimated the tau\ntrident cross sections. Our full $2\\to 4$ calculation, including both coherent\nand incoherent scatterings, reveals that the DUNE near detector will actually\nget a non-negligible number of tau tridents, which is an important background\nto new physics searches. We identify promising kinematic features that may\nallow distinction of tau tridents from the usual neutrino charged-current\nbackground at DUNE, and thus could establish the observation of tau tridents\nfor the first time. We also comment on the detection prospects at other\naccelerator and collider neutrino experiments.",
            "pdf_url": "http://arxiv.org/pdf/2406.20067v1",
            "published": "2024-06-28 17:23:44+00:00",
            "updated": "2024-06-28 17:23:44+00:00"
        },
        {
            "title": "Neural Differentiable Modeling with Diffusion-Based Super-resolution for Two-Dimensional Spatiotemporal Turbulence",
            "authors": "Xiantao Fan, Deepak Akhare, Jian-Xun Wang",
            "summary": "Simulating spatiotemporal turbulence with high fidelity remains a cornerstone\nchallenge in computational fluid dynamics (CFD) due to its intricate multiscale\nnature and prohibitive computational demands. Traditional approaches typically\nemploy closure models, which attempt to represent small-scale features in an\nunresolved manner. However, these methods often sacrifice accuracy and lose\nhigh-frequency/wavenumber information, especially in scenarios involving\ncomplex flow physics. In this paper, we introduce an innovative neural\ndifferentiable modeling framework designed to enhance the predictability and\nefficiency of spatiotemporal turbulence simulations. Our approach features\ndifferentiable hybrid modeling techniques that seamlessly integrate deep neural\nnetworks with numerical PDE solvers within a differentiable programming\nframework, synergizing deep learning with physics-based CFD modeling.\nSpecifically, a hybrid differentiable neural solver is constructed on a coarser\ngrid to capture large-scale turbulent phenomena, followed by the application of\na Bayesian conditional diffusion model that generates small-scale turbulence\nconditioned on large-scale flow predictions. Two innovative hybrid architecture\ndesigns are studied, and their performance is evaluated through comparative\nanalysis against conventional large eddy simulation techniques with\nphysics-based subgrid-scale closures and purely data-driven neural solvers. The\nfindings underscore the potential of the neural differentiable modeling\nframework to significantly enhance the accuracy and computational efficiency of\nturbulence simulations. This study not only demonstrates the efficacy of\nmerging deep learning with physics-based numerical solvers but also sets a new\nprecedent for advanced CFD modeling techniques, highlighting the transformative\nimpact of differentiable programming in scientific computing.",
            "pdf_url": "http://arxiv.org/pdf/2406.20047v1",
            "published": "2024-06-28 17:01:39+00:00",
            "updated": "2024-06-28 17:01:39+00:00"
        },
        {
            "title": "Electrostatics-based particle sampling and approximate inference",
            "authors": "Yongchao Huang",
            "summary": "A new particle-based sampling and approximate inference method, based on\nelectrostatics and Newton mechanics principles, is introduced with theoretical\nground, algorithm design and experimental validation. This method simulates an\ninteracting particle system (IPS) where particles, i.e. the freely-moving\nnegative charges and spatially-fixed positive charges with magnitudes\nproportional to the target distribution, interact with each other via\nattraction and repulsion induced by the resulting electric fields described by\nPoisson's equation. The IPS evolves towards a steady-state where the\ndistribution of negative charges conforms to the target distribution. This\nphysics-inspired method offers deterministic, gradient-free sampling and\ninference, achieving comparable performance as other particle-based and MCMC\nmethods in benchmark tasks of inferring complex densities, Bayesian logistic\nregression and dynamical system identification. A discrete-time, discrete-space\nalgorithmic design, readily extendable to continuous time and space, is\nprovided for usage in more general inference problems occurring in\nprobabilistic machine learning scenarios such as Bayesian inference, generative\nmodelling, and beyond.",
            "pdf_url": "http://arxiv.org/pdf/2406.20044v1",
            "published": "2024-06-28 16:53:06+00:00",
            "updated": "2024-06-28 16:53:06+00:00"
        },
        {
            "title": "On Bjorken sum rule with analytic coupling",
            "authors": "I. R. Gabdrakhmanov, N. A Gramotkov, A. V. Kotikov, O. V. Teryaev, D. A. Volkova, I. A. Zemlyakov",
            "summary": "We present the results of [1], where good agreement was obtained between\ncalculations within the framework of analytic QCD and experimental data on\npolarized Bjorken sum rule. The photoproduction limit was also considered and a\nnew representation of the perturbative contribution to the polarized Bjorken\nsum rule was obtained.",
            "pdf_url": "http://arxiv.org/pdf/2406.20000v1",
            "published": "2024-06-28 15:35:20+00:00",
            "updated": "2024-06-28 15:35:20+00:00"
        },
        {
            "title": "Quantum error cancellation in photonic systems -- undoing photon losses",
            "authors": "Adam Taylor, Gabriele Bressanini, Hyukjoon Kwon, M. S. Kim",
            "summary": "Real photonic devices are subject to photon losses that can decohere quantum\ninformation encoded in the system. In the absence of full fault tolerance,\nquantum error mitigation techniques have been introduced to help manage errors\nin noisy quantum devices. In this work, we introduce an error mitigation\nprotocol inspired by probabilistic error cancellation (a popular error\nmitigation technique in discrete variable systems) for continuous variable\nsystems. We show that our quantum error cancellation protocol can undo photon\nlosses in expectation value estimation tasks. To do this, we analytically\nderive the (non-physical) inverse photon loss channel and decompose it into a\nsum over physically realisable channels with potentially negative coefficients.\nThe bias of our ideal expectation value estimator can be made arbitrarily small\nat the cost of increasing the sampling overhead. The protocol requires a\nnoiseless amplification followed by a series of photon-subtractions. While\nthese operations can be implemented probabilistically, for certain classes of\ninitial state one can avoid the burden of carrying out the amplification and\nphoton-subtractions by leveraging Monte-Carlo methods to give an unbiased\nestimate of the ideal expectation value. We validate our proposed mitigation\nprotocol by simulating the scheme on squeezed vacuum states, cat states and\nentangled coherent states.",
            "pdf_url": "http://arxiv.org/pdf/2403.05252v2",
            "published": "2024-03-08 12:16:43+00:00",
            "updated": "2024-06-28 15:30:31+00:00"
        },
        {
            "title": "Scalable Bayesian uncertainty quantification with data-driven priors for radio interferometric imaging",
            "authors": "Tob\u00edas I. Liaudat, Matthijs Mars, Matthew A. Price, Marcelo Pereyra, Marta M. Betcke, Jason D. McEwen",
            "summary": "Next-generation radio interferometers like the Square Kilometer Array have\nthe potential to unlock scientific discoveries thanks to their unprecedented\nangular resolution and sensitivity. One key to unlocking their potential\nresides in handling the deluge and complexity of incoming data. This challenge\nrequires building radio interferometric imaging methods that can cope with the\nmassive data sizes and provide high-quality image reconstructions with\nuncertainty quantification (UQ). This work proposes a method coined QuantifAI\nto address UQ in radio-interferometric imaging with data-driven (learned)\npriors for high-dimensional settings. Our model, rooted in the Bayesian\nframework, uses a physically motivated model for the likelihood. The model\nexploits a data-driven convex prior, which can encode complex information\nlearned implicitly from simulations and guarantee the log-concavity of the\nposterior. We leverage probability concentration phenomena of high-dimensional\nlog-concave posteriors that let us obtain information about the posterior,\navoiding MCMC sampling techniques. We rely on convex optimisation methods to\ncompute the MAP estimation, which is known to be faster and better scale with\ndimension than MCMC sampling strategies. Our method allows us to compute local\ncredible intervals, i.e., Bayesian error bars, and perform hypothesis testing\nof structure on the reconstructed image. In addition, we propose a novel\nblazing-fast method to compute pixel-wise uncertainties at different scales. We\ndemonstrate our method by reconstructing radio-interferometric images in a\nsimulated setting and carrying out fast and scalable UQ, which we validate with\nMCMC sampling. Our method shows an improved image quality and more meaningful\nuncertainties than the benchmark method based on a sparsity-promoting prior.\nQuantifAI's source code: https://github.com/astro-informatics/QuantifAI.",
            "pdf_url": "http://arxiv.org/pdf/2312.00125v2",
            "published": "2023-11-30 19:00:02+00:00",
            "updated": "2024-06-28 15:17:05+00:00"
        },
        {
            "title": "Tracer dynamics in the active random average process",
            "authors": "Saikat Santra, Prashant Singh, Anupam Kundu",
            "summary": "We investigate the dynamics of tracer particles in the random average process\n(RAP), a single-file system in one dimension. In addition to the position,\nevery particle possesses an internal spin variable $\\sigma (t)$ that can\nalternate between two values, $\\pm 1$, at a constant rate $\\gamma$. Physically,\nthe value of $\\sigma (t)$ dictates the direction of motion of the corresponding\nparticle and for finite $\\gamma$, every particle performs a non-Markovian\nactive dynamics. Herein, we study the effect of this non-Markovianity in the\nfluctuations and correlations of the positions of tracer particles. We\nanalytically show that the variance of the position of a tagged particle grows\nsub-diffusively as $\\sim \\zeta_{\\text{q}} \\sqrt{t}$ at large times for the\nquenched uniform initial condition. While this sub-diffusive growth is\nidentical to that of the Markovian/non-persistent RAP, the coefficient\n$\\zeta_{\\text{q}} $ is rather different and bears the signature of the\npersistent motion of active particles through higher point correlations (unlike\nin the Markovian case). Similarly, for the annealed (steady state) initial\ncondition, we find that the variance scales as $\\sim \\zeta_{\\text{a}} \\sqrt{t}$\nat large times with coefficient $\\zeta_{\\text{a}} $ once again different from\nthe non-persistent case. Although $\\zeta_{\\text{q}}$ and $\\zeta_{\\text{a}} $\nboth individually depart from their Markov counterparts, their ratio\n$\\zeta_{\\text{a}} / \\zeta_{\\text{q}}$ is still equal to $\\sqrt{2}$, a condition\nobserved for other diffusive single-file systems. This condition turns out to\nbe true even in the strongly active regimes as corroborated by extensive\nsimulations and calculations. Finally, we study the correlation between the\npositions of two tagged particles in both quenched uniform and annealed initial\nconditions. We verify all our analytic results by extensive numerical\nsimulations.",
            "pdf_url": "http://arxiv.org/pdf/2307.09908v3",
            "published": "2023-07-19 11:21:51+00:00",
            "updated": "2024-06-28 15:16:17+00:00"
        },
        {
            "title": "Text2Robot: Evolutionary Robot Design from Text Descriptions",
            "authors": "Ryan P. Ringel, Zachary S. Charlick, Jiaxun Liu, Boxi Xia, Boyuan Chen",
            "summary": "Robot design has traditionally been costly and labor-intensive. Despite\nadvancements in automated processes, it remains challenging to navigate a vast\ndesign space while producing physically manufacturable robots. We introduce\nText2Robot, a framework that converts user text specifications and performance\npreferences into physical quadrupedal robots. Within minutes, Text2Robot can\nuse text-to-3D models to provide strong initializations of diverse\nmorphologies. Within a day, our geometric processing algorithms and\nbody-control co-optimization produce a walking robot by explicitly considering\nreal-world electronics and manufacturability. Text2Robot enables rapid\nprototyping and opens new opportunities for robot design with generative\nmodels.",
            "pdf_url": "http://arxiv.org/pdf/2406.19963v1",
            "published": "2024-06-28 14:51:01+00:00",
            "updated": "2024-06-28 14:51:01+00:00"
        },
        {
            "title": "Gauge invariant quantum backreaction in U(1) axion inflation",
            "authors": "Davide Campanella Galanti, Pietro Conzinu, Giovanni Marozzi, Simony Santos da Costa",
            "summary": "We evaluate the quantum backreaction due to a gauge field coupled to a pseudo\nscalar field driving a slow-roll inflationary stage, the so-called axion\ninflation. The backreaction is evaluated for the first time using a gauge\ninvariant approach, going to second order in perturbation theory and taking in\nconsideration inflaton fluctuations as well as scalar perturbations of the\nmetric. Within our gauge invariant, but observer-dependent approach, we\nnaturally consider as physical observer the one comoving with the inflaton\nfield. Considering the effective expansion rate consequent to the gauge fields\nbacreaction, we observe that the backreaction effect becomes significant quite\nrapidly, moving the system out of the perturbative regime and into what is\noften referred to the strong backreaction regime. This behavior also applies to\nthe parameter which dictates the production of the gauge fields. The space-time\ndynamic is initially mainly influenced by the helicity contribution, with a\nsignificant response to the energy density which occurs much later with respect\nto the number of e-folds. As a final result, we see that the evaluated\nbackreaction prolongs the inflationary period much more compared to the\nscenarios previously studied neglecting scalar metric perturbations.",
            "pdf_url": "http://arxiv.org/pdf/2406.19960v1",
            "published": "2024-06-28 14:48:05+00:00",
            "updated": "2024-06-28 14:48:05+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "On the Trade-off between Flatness and Optimization in Distributed Learning",
            "authors": "Ying Cao, Zhaoxian Wu, Kun Yuan, Ali H. Sayed",
            "summary": "This paper proposes a theoretical framework to evaluate and compare the\nperformance of gradient-descent algorithms for distributed learning in relation\nto their behavior around local minima in nonconvex environments. Previous works\nhave noticed that convergence toward flat local minima tend to enhance the\ngeneralization ability of learning algorithms. This work discovers two\ninteresting results. First, it shows that decentralized learning strategies are\nable to escape faster away from local minimizers and favor convergence toward\nflatter minima relative to the centralized solution in the large-batch training\nregime. Second, and importantly, the ultimate classification accuracy is not\nsolely dependent on the flatness of the local minimizer but also on how well a\nlearning algorithm can approach that minimum. In other words, the\nclassification accuracy is a function of both flatness and optimization\nperformance. The paper examines the interplay between the two measures of\nflatness and optimization error closely. One important conclusion is that\ndecentralized strategies of the diffusion type deliver enhanced classification\naccuracy because it strikes a more favorable balance between flatness and\noptimization performance.",
            "pdf_url": "http://arxiv.org/pdf/2406.20006v1",
            "published": "2024-06-28 15:46:08+00:00",
            "updated": "2024-06-28 15:46:08+00:00"
        },
        {
            "title": "Kandinsky 3.0 Technical Report",
            "authors": "Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, Denis Dimitrov",
            "summary": "We present Kandinsky 3.0, a large-scale text-to-image generation model based\non latent diffusion, continuing the series of text-to-image Kandinsky models\nand reflecting our progress to achieve higher quality and realism of image\ngeneration. In this report we describe the architecture of the model, the data\ncollection procedure, the training technique, and the production system for\nuser interaction. We focus on the key components that, as we have identified as\na result of a large number of experiments, had the most significant impact on\nimproving the quality of our model compared to the others. We also describe\nextensions and applications of our model, including super resolution,\ninpainting, image editing, image-to-video generation, and a distilled version\nof Kandinsky 3.0 - Kandinsky 3.1, which does inference in 4 steps of the\nreverse process and 20 times faster without visual quality decrease. By\nside-by-side human preferences comparison, Kandinsky becomes better in text\nunderstanding and works better on specific domains. The code is available at\nhttps://github.com/ai-forever/Kandinsky-3",
            "pdf_url": "http://arxiv.org/pdf/2312.03511v3",
            "published": "2023-12-06 14:13:38+00:00",
            "updated": "2024-06-28 11:23:11+00:00"
        },
        {
            "title": "Deceptive Diffusion: Generating Synthetic Adversarial Examples",
            "authors": "Lucas Beerens, Catherine F. Higham, Desmond J. Higham",
            "summary": "We introduce the concept of deceptive diffusion -- training a generative AI\nmodel to produce adversarial images. Whereas a traditional adversarial attack\nalgorithm aims to perturb an existing image to induce a misclassificaton, the\ndeceptive diffusion model can create an arbitrary number of new, misclassified\nimages that are not directly associated with training or test images. Deceptive\ndiffusion offers the possibility of strengthening defence algorithms by\nproviding adversarial training data at scale, including types of\nmisclassification that are otherwise difficult to find. In our experiments, we\nalso investigate the effect of training on a partially attacked data set. This\nhighlights a new type of vulnerability for generative diffusion models: if an\nattacker is able to stealthily poison a portion of the training data, then the\nresulting diffusion model will generate a similar proportion of misleading\noutputs.",
            "pdf_url": "http://arxiv.org/pdf/2406.19807v1",
            "published": "2024-06-28 10:30:46+00:00",
            "updated": "2024-06-28 10:30:46+00:00"
        },
        {
            "title": "DISCO: Efficient Diffusion Solver for Large-Scale Combinatorial Optimization Problems",
            "authors": "Kexiong Yu, Hang Zhao, Yuhang Huang, Renjiao Yi, Kai Xu, Chenyang Zhu",
            "summary": "Combinatorial Optimization (CO) problems are fundamentally crucial in\nnumerous practical applications across diverse industries, characterized by\nentailing enormous solution space and demanding time-sensitive response.\nDespite significant advancements made by recent neural solvers, their limited\nexpressiveness does not conform well to the multi-modal nature of CO\nlandscapes. While some research has pivoted towards diffusion models, they\nrequire simulating a Markov chain with many steps to produce a sample, which is\ntime-consuming and does not meet the efficiency requirement of real\napplications, especially at scale. We propose DISCO, an efficient DIffusion\nSolver for Combinatorial Optimization problems that excels in both solution\nquality and inference speed. DISCO's efficacy is two-pronged: Firstly, it\nachieves rapid denoising of solutions through an analytically solvable form,\nallowing for direct sampling from the solution space with very few reverse-time\nsteps, thereby drastically reducing inference time. Secondly, DISCO enhances\nsolution quality by restricting the sampling space to a more constrained,\nmeaningful domain guided by solution residues, while still preserving the\ninherent multi-modality of the output probabilistic distributions. DISCO\nachieves state-of-the-art results on very large Traveling Salesman Problems\nwith 10000 nodes and challenging Maximal Independent Set benchmarks, with its\nper-instance denoising time up to 44.8 times faster. Through further combining\na divide-and-conquer strategy, DISCO can be generalized to solve\narbitrary-scale problem instances off the shelf, even outperforming models\ntrained specifically on corresponding scales.",
            "pdf_url": "http://arxiv.org/pdf/2406.19705v1",
            "published": "2024-06-28 07:36:31+00:00",
            "updated": "2024-06-28 07:36:31+00:00"
        },
        {
            "title": "Behavior Generation with Latent Actions",
            "authors": "Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, Lerrel Pinto",
            "summary": "Generative modeling of complex behaviors from labeled datasets has been a\nlongstanding problem in decision making. Unlike language or image generation,\ndecision making requires modeling actions - continuous-valued vectors that are\nmultimodal in their distribution, potentially drawn from uncurated sources,\nwhere generation errors can compound in sequential prediction. A recent class\nof models called Behavior Transformers (BeT) addresses this by discretizing\nactions using k-means clustering to capture different modes. However, k-means\nstruggles to scale for high-dimensional action spaces or long sequences, and\nlacks gradient information, and thus BeT suffers in modeling long-range\nactions. In this work, we present Vector-Quantized Behavior Transformer\n(VQ-BeT), a versatile model for behavior generation that handles multimodal\naction prediction, conditional generation, and partial observations. VQ-BeT\naugments BeT by tokenizing continuous actions with a hierarchical vector\nquantization module. Across seven environments including simulated\nmanipulation, autonomous driving, and robotics, VQ-BeT improves on\nstate-of-the-art models such as BeT and Diffusion Policies. Importantly, we\ndemonstrate VQ-BeT's improved ability to capture behavior modes while\naccelerating inference speed 5x over Diffusion Policies. Videos and code can be\nfound https://sjlee.cc/vq-bet",
            "pdf_url": "http://arxiv.org/pdf/2403.03181v2",
            "published": "2024-03-05 18:19:29+00:00",
            "updated": "2024-06-28 04:15:33+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist",
            "authors": "Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun Wang, Bo An",
            "summary": "Financial trading is a crucial component of the markets, informed by a\nmultimodal information landscape encompassing news, prices, and Kline charts,\nand encompasses diverse tasks such as quantitative trading and high-frequency\ntrading with various assets. While advanced AI techniques like deep learning\nand reinforcement learning are extensively utilized in finance, their\napplication in financial trading tasks often faces challenges due to inadequate\nhandling of multimodal data and limited generalizability across various tasks.\nTo address these challenges, we present FinAgent, a multimodal foundational\nagent with tool augmentation for financial trading. FinAgent's market\nintelligence module processes a diverse range of data-numerical, textual, and\nvisual-to accurately analyze the financial market. Its unique dual-level\nreflection module not only enables rapid adaptation to market dynamics but also\nincorporates a diversified memory retrieval system, enhancing the agent's\nability to learn from historical data and improve decision-making processes.\nThe agent's emphasis on reasoning for actions fosters trust in its financial\ndecisions. Moreover, FinAgent integrates established trading strategies and\nexpert insights, ensuring that its trading approaches are both data-driven and\nrooted in sound financial principles. With comprehensive experiments on 6\nfinancial datasets, including stocks and Crypto, FinAgent significantly\noutperforms 9 state-of-the-art baselines in terms of 6 financial metrics with\nover 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%\nrelative improvement) is achieved on one dataset. Notably, FinAgent is the\nfirst advanced multimodal foundation agent designed for financial trading\ntasks.",
            "pdf_url": "http://arxiv.org/pdf/2402.18485v3",
            "published": "2024-02-28 17:06:54+00:00",
            "updated": "2024-06-28 10:35:56+00:00"
        }
    ]
}