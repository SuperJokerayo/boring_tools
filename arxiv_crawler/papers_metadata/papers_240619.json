{
    "Physics": [
        {
            "title": "Neural Approximate Mirror Maps for Constrained Diffusion Models",
            "authors": "Berthy T. Feng, Ricardo Baptista, Katherine L. Bouman",
            "summary": "Diffusion models excel at creating visually-convincing images, but they often\nstruggle to meet subtle constraints inherent in the training data. Such\nconstraints could be physics-based (e.g., satisfying a PDE), geometric (e.g.,\nrespecting symmetry), or semantic (e.g., including a particular number of\nobjects). When the training data all satisfy a certain constraint, enforcing\nthis constraint on a diffusion model not only improves its\ndistribution-matching accuracy but also makes it more reliable for generating\nvalid synthetic data and solving constrained inverse problems. However,\nexisting methods for constrained diffusion models are inflexible with different\ntypes of constraints. Recent work proposed to learn mirror diffusion models\n(MDMs) in an unconstrained space defined by a mirror map and to impose the\nconstraint with an inverse mirror map, but analytical mirror maps are\nchallenging to derive for complex constraints. We propose neural approximate\nmirror maps (NAMMs) for general constraints. Our approach only requires a\ndifferentiable distance function from the constraint set. We learn an\napproximate mirror map that pushes data into an unconstrained space and a\ncorresponding approximate inverse that maps data back to the constraint set. A\ngenerative model, such as an MDM, can then be trained in the learned mirror\nspace and its samples restored to the constraint set by the inverse map. We\nvalidate our approach on a variety of constraints, showing that compared to an\nunconstrained diffusion model, a NAMM-based MDM substantially improves\nconstraint satisfaction. We also demonstrate how existing diffusion-based\ninverse-problem solvers can be easily applied in the learned mirror space to\nsolve constrained inverse problems.",
            "pdf_url": "http://arxiv.org/pdf/2406.12816v1",
            "published": "2024-06-18 17:36:09+00:00",
            "updated": "2024-06-18 17:36:09+00:00"
        },
        {
            "title": "Predicting the energetic proton flux with a machine learning regression algorithm",
            "authors": "Mirko Stumpo, Monica Laurenza, Simone Benella, Maria Federica Marcucci",
            "summary": "The need of real-time of monitoring and alerting systems for Space Weather\nhazards has grown significantly in the last two decades. One of the most\nimportant challenge for space mission operations and planning is the prediction\nof solar proton events (SPEs). In this context, artificial intelligence and\nmachine learning techniques have opened a new frontier, providing a new\nparadigm for statistical forecasting algorithms. The great majority of these\nmodels aim to predict the occurrence of a SPE, i.e., they are based on the\nclassification approach. In this work we present a simple and efficient machine\nlearning regression algorithm which is able to forecast the energetic proton\nflux up to 1 hour ahead by exploiting features derived from the electron flux\nonly. This approach could be helpful to improve monitoring systems of the\nradiation risk in both deep space and near-Earth environments. The model is\nvery relevant for mission operations and planning, especially when flare\ncharacteristics and source location are not available in real time, as at Mars\ndistance.",
            "pdf_url": "http://arxiv.org/pdf/2406.12730v1",
            "published": "2024-06-18 15:54:50+00:00",
            "updated": "2024-06-18 15:54:50+00:00"
        },
        {
            "title": "Reinforcement-Learning based routing for packet-optical networks with hybrid telemetry",
            "authors": "A. L. Garc\u00eda Navarro, Nataliia Koneva, Alfonso S\u00e1nchez-Maci\u00e1n, Jos\u00e9 Alberto Hern\u00e1ndez, \u00d3scar Gonz\u00e1lez de Dios, J. M. Rivas-Moscoso",
            "summary": "This article provides a methodology and open-source implementation of\nReinforcement Learning algorithms for finding optimal routes in a\npacket-optical network scenario. The algorithm uses measurements provided by\nthe physical layer (pre-FEC bit error rate and propagation delay) and the link\nlayer (link load) to configure a set of latency-based rewards and penalties\nbased on such measurements. Then, the algorithm executes Q-learning based on\nthis set of rewards for finding the optimal routing strategies. It is further\nshown that the algorithm dynamically adapts to changing network conditions by\nre-calculating optimal policies upon either link load changes or link\ndegradation as measured by pre-FEC BER.",
            "pdf_url": "http://arxiv.org/pdf/2406.12602v1",
            "published": "2024-06-18 13:32:12+00:00",
            "updated": "2024-06-18 13:32:12+00:00"
        },
        {
            "title": "TroL: Traversal of Layers for Large Language and Vision Models",
            "authors": "Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro",
            "summary": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.",
            "pdf_url": "http://arxiv.org/pdf/2406.12246v1",
            "published": "2024-06-18 03:42:00+00:00",
            "updated": "2024-06-18 03:42:00+00:00"
        },
        {
            "title": "Quantum Compiling with Reinforcement Learning on a Superconducting Processor",
            "authors": "Z. T. Wang, Qiuhao Chen, Yuxuan Du, Z. H. Yang, Xiaoxia Cai, Kaixuan Huang, Jingning Zhang, Kai Xu, Jun Du, Yinan Li, Yuling Jiao, Xingyao Wu, Wu Liu, Xiliang Lu, Huikai Xu, Yirong Jin, Ruixia Wang, Haifeng Yu, S. P. Zhao",
            "summary": "To effectively implement quantum algorithms on noisy intermediate-scale\nquantum (NISQ) processors is a central task in modern quantum technology. NISQ\nprocessors feature tens to a few hundreds of noisy qubits with limited\ncoherence times and gate operations with errors, so NISQ algorithms naturally\nrequire employing circuits of short lengths via quantum compilation. Here, we\ndevelop a reinforcement learning (RL)-based quantum compiler for a\nsuperconducting processor and demonstrate its capability of discovering novel\nand hardware-amenable circuits with short lengths. We show that for the\nthree-qubit quantum Fourier transformation, a compiled circuit using only seven\nCZ gates with unity circuit fidelity can be achieved. The compiler is also able\nto find optimal circuits under device topological constraints, with lengths\nconsiderably shorter than those by the conventional method. Our study\nexemplifies the codesign of the software with hardware for efficient quantum\ncompilation, offering valuable insights for the advancement of RL-based\ncompilers.",
            "pdf_url": "http://arxiv.org/pdf/2406.12195v1",
            "published": "2024-06-18 01:49:48+00:00",
            "updated": "2024-06-18 01:49:48+00:00"
        },
        {
            "title": "Thermodynamic Transferability in Coarse-Grained Force Fields using Graph Neural Networks",
            "authors": "Emily Shinkle, Aleksandra Pachalieva, Riti Bahl, Sakib Matin, Brendan Gifford, Galen T. Craven, Nicholas Lubbers",
            "summary": "Coarse-graining is a molecular modeling technique in which an atomistic\nsystem is represented in a simplified fashion that retains the most significant\nsystem features that contribute to a target output, while removing the degrees\nof freedom that are less relevant. This reduction in model complexity allows\ncoarse-grained molecular simulations to reach increased spatial and temporal\nscales compared to corresponding all-atom models. A core challenge in\ncoarse-graining is to construct a force field that represents the interactions\nin the new representation in a way that preserves the atomistic-level\nproperties. Many approaches to building coarse-grained force fields have\nlimited transferability between different thermodynamic conditions as a result\nof averaging over internal fluctuations at a specific thermodynamic state\npoint. Here, we use a graph-convolutional neural network architecture, the\nHierarchically Interacting Particle Neural Network with Tensor Sensitivity\n(HIP-NN-TS), to develop a highly automated training pipeline for coarse grained\nforce fields which allows for studying the transferability of coarse-grained\nmodels based on the force-matching approach. We show that this approach not\nonly yields highly accurate force fields, but also that these force fields are\nmore transferable through a variety of thermodynamic conditions. These results\nillustrate the potential of machine learning techniques such as graph neural\nnetworks to improve the construction of transferable coarse-grained force\nfields.",
            "pdf_url": "http://arxiv.org/pdf/2406.12112v1",
            "published": "2024-06-17 21:44:05+00:00",
            "updated": "2024-06-17 21:44:05+00:00"
        },
        {
            "title": "QC-Forest: a Classical-Quantum Algorithm to Provably Speedup Retraining of Random Forest",
            "authors": "Romina Yalovetzky, Niran Kumar, Changhao Li, Marco Pistoia",
            "summary": "Random Forest (RF) is a popular tree-ensemble method for supervised learning,\nprized for its ease of use and flexibility. Online RF models require to account\nfor new training data to maintain model accuracy. This is particularly\nimportant in applications were data is periodically and sequentially generated\nover time in data streams, such as auto-driving systems, and credit card\npayments. In this setting, performing periodic model retraining with the old\nand new data accumulated is beneficial as it fully captures possible drifts in\nthe data distribution over time. However, this is unpractical with\nstate-of-the-art classical algorithms for RF as they scale linearly with the\naccumulated number of samples. We propose QC-Forest, a classical-quantum\nalgorithm designed to time-efficiently retrain RF models in the streaming\nsetting for multi-class classification and regression, achieving a runtime\npoly-logarithmic in the total number of accumulated samples. QC-Forest\nleverages Des-q, a quantum algorithm for single tree construction and\nretraining proposed by Kumar et al. by expanding to multi-class classification,\nas the original proposal was limited to binary classes, and introducing an\nexact classical method to replace an underlying quantum subroutine incurring a\nfinite error, while maintaining the same poly-logarithmic dependence. Finally,\nwe showcase that QC-Forest achieves competitive accuracy in comparison to\nstate-of-the-art RF methods on widely used benchmark datasets with up to 80,000\nsamples, while significantly speeding up the model retrain.",
            "pdf_url": "http://arxiv.org/pdf/2406.12008v1",
            "published": "2024-06-17 18:21:03+00:00",
            "updated": "2024-06-17 18:21:03+00:00"
        },
        {
            "title": "Modeling, Inference, and Prediction in Mobility-Based Compartmental Models for Epidemiology",
            "authors": "Ning Jiang, Weiqi Chu, Yao Li",
            "summary": "Classical compartmental models in epidemiology often struggle to accurately\ncapture real-world dynamics due to their inability to address the inherent\nheterogeneity of populations. In this paper, we introduce a novel approach that\nincorporates heterogeneity through a mobility variable, transforming the\ntraditional ODE system into a system of integro-differential equations that\ndescribe the dynamics of population densities across different compartments.\nOur results show that, for the same basic reproduction number, our\nmobility-based model predicts a smaller final pandemic size compared to classic\ncompartmental models, whose population densities are represented as Dirac delta\nfunctions in our density-based framework. This addresses the overestimation\nissue common in many classical models. Additionally, we demonstrate that the\ntime series of the infected population is sufficient to uniquely identify the\nmobility distribution. We reconstruct this distribution using a\nmachine-learning-based framework, providing both theoretical and algorithmic\nsupport to effectively constrain the mobility-based model with real-world data.",
            "pdf_url": "http://arxiv.org/pdf/2406.12002v1",
            "published": "2024-06-17 18:13:57+00:00",
            "updated": "2024-06-17 18:13:57+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Evaluating the design space of diffusion-based generative models",
            "authors": "Yuqing Wang, Ye He, Molei Tao",
            "summary": "Most existing theoretical investigations of the accuracy of diffusion models,\nalbeit significant, assume the score function has been approximated to a\ncertain accuracy, and then use this a priori bound to control the error of\ngeneration. This article instead provides a first quantitative understanding of\nthe whole generation process, i.e., both training and sampling. More precisely,\nit conducts a non-asymptotic convergence analysis of denoising score matching\nunder gradient descent. In addition, a refined sampling error analysis for\nvariance exploding models is also provided. The combination of these two\nresults yields a full error analysis, which elucidates (again, but this time\ntheoretically) how to design the training and sampling processes for effective\ngeneration. For instance, our theory implies a preference toward noise\ndistribution and loss weighting that qualitatively agree with the ones used in\n[Karras et al. 2022]. It also provides some perspectives on why the time and\nvariance schedule used in [Karras et al. 2022] could be better tuned than the\npioneering version in [Song et al. 2020].",
            "pdf_url": "http://arxiv.org/pdf/2406.12839v1",
            "published": "2024-06-18 17:56:10+00:00",
            "updated": "2024-06-18 17:56:10+00:00"
        },
        {
            "title": "Influence Maximization via Graph Neural Bandits",
            "authors": "Yuting Feng, Vincent Y. F. Tan, Bogdan Cautis",
            "summary": "We consider a ubiquitous scenario in the study of Influence Maximization\n(IM), in which there is limited knowledge about the topology of the diffusion\nnetwork. We set the IM problem in a multi-round diffusion campaign, aiming to\nmaximize the number of distinct users that are influenced. Leveraging the\ncapability of bandit algorithms to effectively balance the objectives of\nexploration and exploitation, as well as the expressivity of neural networks,\nour study explores the application of neural bandit algorithms to the IM\nproblem. We propose the framework IM-GNB (Influence Maximization with Graph\nNeural Bandits), where we provide an estimate of the users' probabilities of\nbeing influenced by influencers (also known as diffusion seeds). This initial\nestimate forms the basis for constructing both an exploitation graph and an\nexploration one. Subsequently, IM-GNB handles the exploration-exploitation\ntradeoff, by selecting seed nodes in real-time using Graph Convolutional\nNetworks (GCN), in which the pre-estimated graphs are employed to refine the\ninfluencers' estimated rewards in each contextual setting. Through extensive\nexperiments on two large real-world datasets, we demonstrate the effectiveness\nof IM-GNB compared with other baseline methods, significantly improving the\nspread outcome of such diffusion campaigns, when the underlying network is\nunknown.",
            "pdf_url": "http://arxiv.org/pdf/2406.12835v1",
            "published": "2024-06-18 17:54:33+00:00",
            "updated": "2024-06-18 17:54:33+00:00"
        },
        {
            "title": "Extracting Training Data from Unconditional Diffusion Models",
            "authors": "Yunhao Chen, Xingjun Ma, Difan Zou, Yu-Gang Jiang",
            "summary": "As diffusion probabilistic models (DPMs) are being employed as mainstream\nmodels for generative artificial intelligence (AI), the study of their\nmemorization of the raw training data has attracted growing attention. Existing\nworks in this direction aim to establish an understanding of whether or to what\nextent DPMs learn by memorization. Such an understanding is crucial for\nidentifying potential risks of data leakage and copyright infringement in\ndiffusion models and, more importantly, for more controllable generation and\ntrustworthy application of Artificial Intelligence Generated Content (AIGC).\nWhile previous works have made important observations of when DPMs are prone to\nmemorization, these findings are mostly empirical, and the developed data\nextraction methods only work for conditional diffusion models. In this work, we\naim to establish a theoretical understanding of memorization in DPMs with 1) a\nmemorization metric for theoretical analysis, 2) an analysis of conditional\nmemorization with informative and random labels, and 3) two better evaluation\nmetrics for measuring memorization. Based on the theoretical analysis, we\nfurther propose a novel data extraction method called \\textbf{Surrogate\ncondItional Data Extraction (SIDE)} that leverages a classifier trained on\ngenerated data as a surrogate condition to extract training data directly from\nunconditional diffusion models. Our empirical results demonstrate that SIDE can\nextract training data from diffusion models where previous methods fail, and it\nis on average over 50\\% more effective across different scales of the CelebA\ndataset.",
            "pdf_url": "http://arxiv.org/pdf/2406.12752v1",
            "published": "2024-06-18 16:20:12+00:00",
            "updated": "2024-06-18 16:20:12+00:00"
        },
        {
            "title": "Learning Diffusion at Lightspeed",
            "authors": "Antonio Terpin, Nicolas Lanzetti, Florian D\u00f6rfler",
            "summary": "Diffusion regulates a phenomenal number of natural processes and the dynamics\nof many successful generative models. Existing models to learn the diffusion\nterms from observational data rely on complex bilevel optimization problems and\nproperly model only the drift of the system. We propose a new simple model,\nJKOnet*, which bypasses altogether the complexity of existing architectures\nwhile presenting significantly enhanced representational capacity: JKOnet*\nrecovers the potential, interaction, and internal energy components of the\nunderlying diffusion process. JKOnet* minimizes a simple quadratic loss, runs\nat lightspeed, and drastically outperforms other baselines in practice.\nAdditionally, JKOnet* provides a closed-form optimal solution for linearly\nparametrized functionals. Our methodology is based on the interpretation of\ndiffusion processes as energy-minimizing trajectories in the probability space\nvia the so-called JKO scheme, which we study via its first-order optimality\nconditions, in light of few-weeks-old advancements in optimization in the\nprobability space.",
            "pdf_url": "http://arxiv.org/pdf/2406.12616v1",
            "published": "2024-06-18 13:44:07+00:00",
            "updated": "2024-06-18 13:44:07+00:00"
        }
    ],
    "Quantitative Finance": [
        {
            "title": "The Limits of Pure Exploration in POMDPs: When the Observation Entropy is Enough",
            "authors": "Riccardo Zamboni, Duilio Cirino, Marcello Restelli, Mirco Mutti",
            "summary": "The problem of pure exploration in Markov decision processes has been cast as\nmaximizing the entropy over the state distribution induced by the agent's\npolicy, an objective that has been extensively studied. However, little\nattention has been dedicated to state entropy maximization under partial\nobservability, despite the latter being ubiquitous in applications, e.g.,\nfinance and robotics, in which the agent only receives noisy observations of\nthe true state governing the system's dynamics. How can we address state\nentropy maximization in those domains? In this paper, we study the simple\napproach of maximizing the entropy over observations in place of true latent\nstates. First, we provide lower and upper bounds to the approximation of the\ntrue state entropy that only depends on some properties of the observation\nfunction. Then, we show how knowledge of the latter can be exploited to compute\na principled regularization of the observation entropy to improve performance.\nWith this work, we provide both a flexible approach to bring advances in state\nentropy maximization to the POMDP setting and a theoretical characterization of\nits intrinsic limits.",
            "pdf_url": "http://arxiv.org/pdf/2406.12795v1",
            "published": "2024-06-18 17:00:13+00:00",
            "updated": "2024-06-18 17:00:13+00:00"
        },
        {
            "title": "Bridging Design Gaps: A Parametric Data Completion Approach With Graph Guided Diffusion Models",
            "authors": "Rui Zhou, Chenyang Yuan, Frank Permenter, Yanxia Zhang, Nikos Arechiga, Matt Klenk, Faez Ahmed",
            "summary": "This study introduces a generative imputation model leveraging graph\nattention networks and tabular diffusion models for completing missing\nparametric data in engineering designs. This model functions as an AI design\nco-pilot, providing multiple design options for incomplete designs, which we\ndemonstrate using the bicycle design CAD dataset. Through comparative\nevaluations, we demonstrate that our model significantly outperforms existing\nclassical methods, such as MissForest, hotDeck, PPCA, and tabular generative\nmethod TabCSDI in both the accuracy and diversity of imputation options.\nGenerative modeling also enables a broader exploration of design possibilities,\nthereby enhancing design decision-making by allowing engineers to explore a\nvariety of design completions. The graph model combines GNNs with the\nstructural information contained in assembly graphs, enabling the model to\nunderstand and predict the complex interdependencies between different design\nparameters. The graph model helps accurately capture and impute complex\nparametric interdependencies from an assembly graph, which is key for design\nproblems. By learning from an existing dataset of designs, the imputation\ncapability allows the model to act as an intelligent assistant that\nautocompletes CAD designs based on user-defined partial parametric design,\neffectively bridging the gap between ideation and realization. The proposed\nwork provides a pathway to not only facilitate informed design decisions but\nalso promote creative exploration in design.",
            "pdf_url": "http://arxiv.org/pdf/2406.11934v1",
            "published": "2024-06-17 16:03:17+00:00",
            "updated": "2024-06-17 16:03:17+00:00"
        },
        {
            "title": "Score-fPINN: Fractional Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck-Levy Equations",
            "authors": "Zheyuan Hu, Zhongqiang Zhang, George Em Karniadakis, Kenji Kawaguchi",
            "summary": "We introduce an innovative approach for solving high-dimensional\nFokker-Planck-L\\'evy (FPL) equations in modeling non-Brownian processes across\ndisciplines such as physics, finance, and ecology. We utilize a fractional\nscore function and Physical-informed neural networks (PINN) to lift the curse\nof dimensionality (CoD) and alleviate numerical overflow from exponentially\ndecaying solutions with dimensions. The introduction of a fractional score\nfunction allows us to transform the FPL equation into a second-order partial\ndifferential equation without fractional Laplacian and thus can be readily\nsolved with standard physics-informed neural networks (PINNs). We propose two\nmethods to obtain a fractional score function: fractional score matching (FSM)\nand score-fPINN for fitting the fractional score function. While FSM is more\ncost-effective, it relies on known conditional distributions. On the other\nhand, score-fPINN is independent of specific stochastic differential equations\n(SDEs) but requires evaluating the PINN model's derivatives, which may be more\ncostly. We conduct our experiments on various SDEs and demonstrate numerical\nstability and effectiveness of our method in dealing with high-dimensional\nproblems, marking a significant advancement in addressing the CoD in FPL\nequations.",
            "pdf_url": "http://arxiv.org/pdf/2406.11676v1",
            "published": "2024-06-17 15:57:23+00:00",
            "updated": "2024-06-17 15:57:23+00:00"
        },
        {
            "title": "Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms",
            "authors": "Vaneet Aggarwal, Washim Uddin Mondal, Qinbo Bai",
            "summary": "Reinforcement Learning (RL) serves as a versatile framework for sequential\ndecision-making, finding applications across diverse domains such as robotics,\nautonomous driving, recommendation systems, supply chain optimization, biology,\nmechanics, and finance. The primary objective in these applications is to\nmaximize the average reward. Real-world scenarios often necessitate adherence\nto specific constraints during the learning process.\n  This monograph focuses on the exploration of various model-based and\nmodel-free approaches for Constrained RL within the context of average reward\nMarkov Decision Processes (MDPs). The investigation commences with an\nexamination of model-based strategies, delving into two foundational methods -\noptimism in the face of uncertainty and posterior sampling. Subsequently, the\ndiscussion transitions to parametrized model-free approaches, where the\nprimal-dual policy gradient-based algorithm is explored as a solution for\nconstrained MDPs. The monograph provides regret guarantees and analyzes\nconstraint violation for each of the discussed setups.\n  For the above exploration, we assume the underlying MDP to be ergodic.\nFurther, this monograph extends its discussion to encompass results tailored\nfor weakly communicating MDPs, thereby broadening the scope of its findings and\ntheir relevance to a wider range of practical scenarios.",
            "pdf_url": "http://arxiv.org/pdf/2406.11481v1",
            "published": "2024-06-17 12:46:02+00:00",
            "updated": "2024-06-17 12:46:02+00:00"
        },
        {
            "title": "Trading Devil: Robust backdoor attack via Stochastic investment models and Bayesian approach",
            "authors": "Orson Mengara",
            "summary": "With the growing use of voice-activated systems and speech recognition\ntechnologies, the danger of backdoor attacks on audio data has grown\nsignificantly. This research looks at a specific type of attack, known as a\nStochastic investment-based backdoor attack (MarketBack), in which adversaries\nstrategically manipulate the stylistic properties of audio to fool speech\nrecognition systems. The security and integrity of machine learning models are\nseriously threatened by backdoor attacks, in order to maintain the reliability\nof audio applications and systems, the identification of such attacks becomes\ncrucial in the context of audio data. Experimental results demonstrated that\nMarketBack is feasible to achieve an average attack success rate close to 100%\nin seven victim models when poisoning less than 1% of the training data.",
            "pdf_url": "http://arxiv.org/pdf/2406.10719v1",
            "published": "2024-06-15 19:12:00+00:00",
            "updated": "2024-06-15 19:12:00+00:00"
        }
    ]
}