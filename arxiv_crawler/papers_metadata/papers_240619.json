{
    "Physics": [
        {
            "title": "Latent Intuitive Physics: Learning to Transfer Hidden Physics from A 3D Video",
            "authors": "Xiangming Zhu, Huayu Deng, Haochen Yuan, Yunbo Wang, Xiaokang Yang",
            "summary": "We introduce latent intuitive physics, a transfer learning framework for\nphysics simulation that can infer hidden properties of fluids from a single 3D\nvideo and simulate the observed fluid in novel scenes. Our key insight is to\nuse latent features drawn from a learnable prior distribution conditioned on\nthe underlying particle states to capture the invisible and complex physical\nproperties. To achieve this, we train a parametrized prior learner given visual\nobservations to approximate the visual posterior of inverse graphics, and both\nthe particle states and the visual posterior are obtained from a learned neural\nrenderer. The converged prior learner is embedded in our probabilistic physics\nengine, allowing us to perform novel simulations on unseen geometries,\nboundaries, and dynamics without knowledge of the true physical parameters. We\nvalidate our model in three ways: (i) novel scene simulation with the learned\nvisual-world physics, (ii) future prediction of the observed fluid dynamics,\nand (iii) supervised particle simulation. Our model demonstrates strong\nperformance in all three tasks.",
            "pdf_url": "http://arxiv.org/pdf/2406.12769v1",
            "published": "2024-06-18 16:37:44+00:00",
            "updated": "2024-06-18 16:37:44+00:00"
        },
        {
            "title": "Research on Dangerous Flight Weather Prediction based on Machine Learning",
            "authors": "Haoxing Liu, Renjie Xie, Haoshen Qin, Yizhou Li",
            "summary": "With the continuous expansion of the scale of air transport, the demand for\naviation meteorological support also continues to grow. The impact of hazardous\nweather on flight safety is critical. How to effectively use meteorological\ndata to improve the early warning capability of flight dangerous weather and\nensure the safe flight of aircraft is the primary task of aviation\nmeteorological services. In this work, support vector machine (SVM) models are\nused to predict hazardous flight weather, especially for meteorological\nconditions with high uncertainty such as storms and turbulence. SVM is a\nsupervised learning method that distinguishes between different classes of data\nby finding optimal decision boundaries in a high-dimensional space. In order to\nmeet the needs of this study, we chose the radial basis function (RBF) as the\nkernel function, which helps to deal with nonlinear problems and enables the\nmodel to better capture complex meteorological data structures. During the\nmodel training phase, we used historical meteorological observations from\nmultiple weather stations, including temperature, humidity, wind speed, wind\ndirection, and other meteorological indicators closely related to flight\nsafety. From this data, the SVM model learns how to distinguish between normal\nand dangerous flight weather conditions.",
            "pdf_url": "http://arxiv.org/pdf/2406.12298v1",
            "published": "2024-06-18 06:08:15+00:00",
            "updated": "2024-06-18 06:08:15+00:00"
        },
        {
            "title": "DustNet: skillful neural network predictions of Saharan dust",
            "authors": "Trish E. Nowak, Andy T. Augousti, Benno I. Simmons, Stefan Siegert",
            "summary": "Suspended in the atmosphere are millions of tonnes of mineral dust which\ninteracts with weather and climate. Accurate representation of mineral dust in\nweather models is vital, yet remains challenging. Large scale weather models\nuse high power supercomputers and take hours to complete the forecast. Such\ncomputational burden allows them to only include monthly climatological means\nof mineral dust as input states inhibiting their forecasting accuracy. Here, we\nintroduce DustNet a simple, accurate and super fast forecasting model for\n24-hours ahead predictions of aerosol optical depth AOD. DustNet trains in less\nthan 8 minutes and creates predictions in 2 seconds on a desktop computer.\nCreated by DustNet predictions outperform the state-of-the-art physics-based\nmodel on coarse 1 x 1 degree resolution at 95% of grid locations when compared\nto ground truth satellite data. Our results show DustNet has a potential for\nfast and accurate AOD forecasting which could transform our understanding of\ndust impacts on weather patterns.",
            "pdf_url": "http://arxiv.org/pdf/2406.11754v1",
            "published": "2024-06-17 17:15:30+00:00",
            "updated": "2024-06-17 17:15:30+00:00"
        },
        {
            "title": "Attention-Based Deep Reinforcement Learning for Qubit Allocation in Modular Quantum Architectures",
            "authors": "Enrico Russo, Maurizio Palesi, Davide Patti, Giuseppe Ascia, Vincenzo Catania",
            "summary": "Modular, distributed and multi-core architectures are currently considered a\npromising approach for scalability of quantum computing systems. The\nintegration of multiple Quantum Processing Units necessitates classical and\nquantum-coherent communication, introducing challenges related to noise and\nquantum decoherence in quantum state transfers between cores. Optimizing\ncommunication becomes imperative, and the compilation and mapping of quantum\ncircuits onto physical qubits must minimize state transfers while adhering to\narchitectural constraints. The compilation process, inherently an NP-hard\nproblem, demands extensive search times even with a small number of qubits to\nbe solved to optimality. To address this challenge efficiently, we advocate for\nthe utilization of heuristic mappers that can rapidly generate solutions. In\nthis work, we propose a novel approach employing Deep Reinforcement Learning\n(DRL) methods to learn these heuristics for a specific multi-core architecture.\nOur DRL agent incorporates a Transformer encoder and Graph Neural Networks. It\nencodes quantum circuits using self-attention mechanisms and produce outputs\nthrough an attention-based pointer mechanism that directly signifies the\nprobability of matching logical qubits with physical cores. This enables the\nselection of optimal cores for logical qubits efficiently. Experimental\nevaluations show that the proposed method can outperform baseline approaches in\nterms of reducing inter-core communications and minimizing online\ntime-to-solution. This research contributes to the advancement of scalable\nquantum computing systems by introducing a novel learning-based heuristic\napproach for efficient quantum circuit compilation and mapping.",
            "pdf_url": "http://arxiv.org/pdf/2406.11452v1",
            "published": "2024-06-17 12:09:11+00:00",
            "updated": "2024-06-17 12:09:11+00:00"
        },
        {
            "title": "WeatherQA: Can Multimodal Language Models Reason about Severe Weather?",
            "authors": "Chengqian Ma, Zhanxiang Hua, Alexandra Anderson-Frey, Vikram Iyer, Xin Liu, Lianhui Qin",
            "summary": "Severe convective weather events, such as hail, tornadoes, and thunderstorms,\noften occur quickly yet cause significant damage, costing billions of dollars\nevery year. This highlights the importance of forecasting severe weather\nthreats hours in advance to better prepare meteorologists and residents in\nat-risk areas. Can modern large foundation models perform such forecasting?\nExisting weather benchmarks typically focus only on predicting time-series\nchanges in certain weather parameters (e.g., temperature, moisture) with\ntext-only features. In this work, we introduce WeatherQA, the first multimodal\ndataset designed for machines to reason about complex combinations of weather\nparameters (a.k.a., ingredients) and predict severe weather in real-world\nscenarios. The dataset includes over 8,000 (multi-images, text) pairs for\ndiverse severe weather events. Each pair contains rich information crucial for\nforecasting -- the images describe the ingredients capturing environmental\ninstability, surface observations, and radar reflectivity, and the text\ncontains forecast analyses written by human experts. With WeatherQA, we\nevaluate state-of-the-art vision language models , including GPT4, Claude3,\nGemini-1.5, and a fine-tuned Llama3-based VLM, by designing two challenging\ntasks: (1) multi-choice QA for predicting affected area and (2) classification\nof the development potential of severe convection. These tasks require deep\nunderstanding of domain knowledge (e.g., atmospheric dynamics) and complex\nreasoning over multimodal data (e.g., interactions between weather parameters).\nWe show a substantial gap between the strongest VLM, GPT4o, and human\nreasoning. Our comprehensive case study with meteorologists further reveals the\nweaknesses of the models, suggesting that better training and data integration\nare necessary to bridge this gap. WeatherQA link:\nhttps://github.com/chengqianma/WeatherQA.",
            "pdf_url": "http://arxiv.org/pdf/2406.11217v1",
            "published": "2024-06-17 05:23:18+00:00",
            "updated": "2024-06-17 05:23:18+00:00"
        },
        {
            "title": "Physics-Informed Deep Learning and Partial Transfer Learning for Bearing Fault Diagnosis in the Presence of Highly Missing Data",
            "authors": "Mohammadreza Kavianpour, Parisa Kavianpour, Amin Ramezani",
            "summary": "One of the most significant obstacles in bearing fault diagnosis is a lack of\nlabeled data for various fault types. Also, sensor-acquired data frequently\nlack labels and have a large amount of missing data. This paper tackles these\nissues by presenting the PTPAI method, which uses a physics-informed deep\nlearning-based technique to generate synthetic labeled data. Labeled synthetic\ndata makes up the source domain, whereas unlabeled data with missing data is\npresent in the target domain. Consequently, imbalanced class problems and\npartial-set fault diagnosis hurdles emerge. To address these challenges, the\nRF-Mixup approach is used to handle imbalanced classes. As domain adaptation\nstrategies, the MK-MMSD and CDAN are employed to mitigate the disparity in\ndistribution between synthetic and actual data. Furthermore, the partial-set\nchallenge is tackled by applying weighting methods at the class and instance\nlevels. Experimental outcomes on the CWRU and JNU datasets indicate that the\nproposed approach effectively addresses these problems.",
            "pdf_url": "http://arxiv.org/pdf/2406.11023v1",
            "published": "2024-06-16 17:36:53+00:00",
            "updated": "2024-06-16 17:36:53+00:00"
        },
        {
            "title": "SPEAR: Receiver-to-Receiver Acoustic Neural Warping Field",
            "authors": "Yuhang He, Shitong Xu, Jia-Xing Zhong, Sangyun Shin, Niki Trigoni, Andrew Markham",
            "summary": "We present SPEAR, a continuous receiver-to-receiver acoustic neural warping\nfield for spatial acoustic effects prediction in an acoustic 3D space with a\nsingle stationary audio source. Unlike traditional source-to-receiver modelling\nmethods that require prior space acoustic properties knowledge to rigorously\nmodel audio propagation from source to receiver, we propose to predict by\nwarping the spatial acoustic effects from one reference receiver position to\nanother target receiver position, so that the warped audio essentially\naccommodates all spatial acoustic effects belonging to the target position.\nSPEAR can be trained in a data much more readily accessible manner, in which we\nsimply ask two robots to independently record spatial audio at different\npositions. We further theoretically prove the universal existence of the\nwarping field if and only if one audio source presents. Three physical\nprinciples are incorporated to guide SPEAR network design, leading to the\nlearned warping field physically meaningful. We demonstrate SPEAR superiority\non both synthetic, photo-realistic and real-world dataset, showing the huge\npotential of SPEAR to various down-stream robotic tasks.",
            "pdf_url": "http://arxiv.org/pdf/2406.11006v1",
            "published": "2024-06-16 16:40:26+00:00",
            "updated": "2024-06-16 16:40:26+00:00"
        },
        {
            "title": "Diffusion Models Are Promising for Ab Initio Structure Solutions from Nanocrystalline Powder Diffraction Data",
            "authors": "Gabe Guo, Tristan Saidi, Maxwell Terban, Simon JL Billinge, Hod Lipson",
            "summary": "A major challenge in materials science is the determination of the structure\nof nanometer sized objects. Here we present a novel approach that uses a\ngenerative machine learning model based on a Diffusion model that is trained on\n45,229 known structures. The model factors both the measured diffraction\npattern as well as relevant statistical priors on the unit cell of atomic\ncluster structures. Conditioned only on the chemical formula and the\ninformation-scarce finite-size broadened powder diffraction pattern, we find\nthat our model, PXRDnet, can successfully solve simulated nanocrystals as small\nas 10 angstroms across 200 materials of varying symmetry and complexity,\nincluding structures from all seven crystal systems. We show that our model can\ndetermine structural solutions with up to $81.5\\%$ accuracy, as measured by\nstructural correlation. Furthermore, PXRDnet is capable of solving structures\nfrom noisy diffraction patterns gathered in real-world experiments. We suggest\nthat data driven approaches, bootstrapped from theoretical simulation, will\nultimately provide a path towards determining the structure of previously\nunsolved nano-materials.",
            "pdf_url": "http://arxiv.org/pdf/2406.10796v1",
            "published": "2024-06-16 03:45:03+00:00",
            "updated": "2024-06-16 03:45:03+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Influence Maximization via Graph Neural Bandits",
            "authors": "Yuting Feng, Vincent Y. F. Tan, Bogdan Cautis",
            "summary": "We consider a ubiquitous scenario in the study of Influence Maximization\n(IM), in which there is limited knowledge about the topology of the diffusion\nnetwork. We set the IM problem in a multi-round diffusion campaign, aiming to\nmaximize the number of distinct users that are influenced. Leveraging the\ncapability of bandit algorithms to effectively balance the objectives of\nexploration and exploitation, as well as the expressivity of neural networks,\nour study explores the application of neural bandit algorithms to the IM\nproblem. We propose the framework IM-GNB (Influence Maximization with Graph\nNeural Bandits), where we provide an estimate of the users' probabilities of\nbeing influenced by influencers (also known as diffusion seeds). This initial\nestimate forms the basis for constructing both an exploitation graph and an\nexploration one. Subsequently, IM-GNB handles the exploration-exploitation\ntradeoff, by selecting seed nodes in real-time using Graph Convolutional\nNetworks (GCN), in which the pre-estimated graphs are employed to refine the\ninfluencers' estimated rewards in each contextual setting. Through extensive\nexperiments on two large real-world datasets, we demonstrate the effectiveness\nof IM-GNB compared with other baseline methods, significantly improving the\nspread outcome of such diffusion campaigns, when the underlying network is\nunknown.",
            "pdf_url": "http://arxiv.org/pdf/2406.12835v1",
            "published": "2024-06-18 17:54:33+00:00",
            "updated": "2024-06-18 17:54:33+00:00"
        },
        {
            "title": "Sparsifying dimensionality reduction of PDE solution data with Bregman learning",
            "authors": "Tjeerd Jan Heeringa, Christoph Brune, Mengwu Guo",
            "summary": "Classical model reduction techniques project the governing equations onto a\nlinear subspace of the original state space. More recent data-driven techniques\nuse neural networks to enable nonlinear projections. Whilst those often enable\nstronger compression, they may have redundant parameters and lead to suboptimal\nlatent dimensionality. To overcome these, we propose a multistep algorithm that\ninduces sparsity in the encoder-decoder networks for effective reduction in the\nnumber of parameters and additional compression of the latent space. This\nalgorithm starts with sparsely initialized a network and training it using\nlinearized Bregman iterations. These iterations have been very successful in\ncomputer vision and compressed sensing tasks, but have not yet been used for\nreduced-order modelling. After the training, we further compress the latent\nspace dimensionality by using a form of proper orthogonal decomposition. Last,\nwe use a bias propagation technique to change the induced sparsity into an\neffective reduction of parameters. We apply this algorithm to three\nrepresentative PDE models: 1D diffusion, 1D advection, and 2D\nreaction-diffusion. Compared to conventional training methods like Adam, the\nproposed method achieves similar accuracy with 30% less parameters and a\nsignificantly smaller latent space.",
            "pdf_url": "http://arxiv.org/pdf/2406.12672v1",
            "published": "2024-06-18 14:45:30+00:00",
            "updated": "2024-06-18 14:45:30+00:00"
        },
        {
            "title": "Variational Distillation of Diffusion Policies into Mixture of Experts",
            "authors": "Hongyi Zhou, Denis Blessing, Ge Li, Onur Celik, Xiaogang Jia, Gerhard Neumann, Rudolf Lioutikov",
            "summary": "This work introduces Variational Diffusion Distillation (VDD), a novel method\nthat distills denoising diffusion policies into Mixtures of Experts (MoE)\nthrough variational inference. Diffusion Models are the current\nstate-of-the-art in generative modeling due to their exceptional ability to\naccurately learn and represent complex, multi-modal distributions. This ability\nallows Diffusion Models to replicate the inherent diversity in human behavior,\nmaking them the preferred models in behavior learning such as Learning from\nHuman Demonstrations (LfD). However, diffusion models come with some drawbacks,\nincluding the intractability of likelihoods and long inference times due to\ntheir iterative sampling process. The inference times, in particular, pose a\nsignificant challenge to real-time applications such as robot control. In\ncontrast, MoEs effectively address the aforementioned issues while retaining\nthe ability to represent complex distributions but are notoriously difficult to\ntrain. VDD is the first method that distills pre-trained diffusion models into\nMoE models, and hence, combines the expressiveness of Diffusion Models with the\nbenefits of Mixture Models. Specifically, VDD leverages a decompositional upper\nbound of the variational objective that allows the training of each expert\nseparately, resulting in a robust optimization scheme for MoEs. VDD\ndemonstrates across nine complex behavior learning tasks, that it is able to:\ni) accurately distill complex distributions learned by the diffusion model, ii)\noutperform existing state-of-the-art distillation methods, and iii) surpass\nconventional methods for training MoE.",
            "pdf_url": "http://arxiv.org/pdf/2406.12538v1",
            "published": "2024-06-18 12:15:05+00:00",
            "updated": "2024-06-18 12:15:05+00:00"
        },
        {
            "title": "COT Flow: Learning Optimal-Transport Image Sampling and Editing by Contrastive Pairs",
            "authors": "Xinrui Zu, Qian Tao",
            "summary": "Diffusion models have demonstrated strong performance in sampling and editing\nmulti-modal data with high generation quality, yet they suffer from the\niterative generation process which is computationally expensive and slow. In\naddition, most methods are constrained to generate data from Gaussian noise,\nwhich limits their sampling and editing flexibility. To overcome both\ndisadvantages, we present Contrastive Optimal Transport Flow (COT Flow), a new\nmethod that achieves fast and high-quality generation with improved zero-shot\nediting flexibility compared to previous diffusion models. Benefiting from\noptimal transport (OT), our method has no limitation on the prior distribution,\nenabling unpaired image-to-image (I2I) translation and doubling the editable\nspace (at both the start and end of the trajectory) compared to other zero-shot\nediting methods. In terms of quality, COT Flow can generate competitive results\nin merely one step compared to previous state-of-the-art unpaired\nimage-to-image (I2I) translation methods. To highlight the advantages of COT\nFlow through the introduction of OT, we introduce the COT Editor to perform\nuser-guided editing with excellent flexibility and quality. The code will be\nreleased at https://github.com/zuxinrui/cot_flow.",
            "pdf_url": "http://arxiv.org/pdf/2406.12140v1",
            "published": "2024-06-17 23:02:20+00:00",
            "updated": "2024-06-17 23:02:20+00:00"
        },
        {
            "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
            "authors": "Yulai Zhao, Masatoshi Uehara, Gabriele Scalia, Tommaso Biancalani, Sergey Levine, Ehsan Hajiramezanali",
            "summary": "Diffusion models are powerful generative models that allow for precise\ncontrol over the characteristics of the generated samples. While these\ndiffusion models trained on large datasets have achieved success, there is\noften a need to introduce additional controls in downstream fine-tuning\nprocesses, treating these powerful models as pre-trained diffusion models. This\nwork presents a novel method based on reinforcement learning (RL) to add\nadditional controls, leveraging an offline dataset comprising inputs and\ncorresponding labels. We formulate this task as an RL problem, with the\nclassifier learned from the offline dataset and the KL divergence against\npre-trained models serving as the reward functions. We introduce our method,\n$\\textbf{CTRL}$ ($\\textbf{C}$onditioning pre-$\\textbf{T}$rained diffusion\nmodels with $\\textbf{R}$einforcement $\\textbf{L}$earning), which produces\nsoft-optimal policies that maximize the abovementioned reward functions. We\nformally demonstrate that our method enables sampling from the conditional\ndistribution conditioned on additional controls during inference. Our RL-based\napproach offers several advantages over existing methods. Compared to commonly\nused classifier-free guidance, our approach improves sample efficiency, and can\ngreatly simplify offline dataset construction by exploiting conditional\nindependence between the inputs and additional controls. Furthermore, unlike\nclassifier guidance, we avoid the need to train classifiers from intermediate\nstates to additional controls.",
            "pdf_url": "http://arxiv.org/pdf/2406.12120v1",
            "published": "2024-06-17 22:00:26+00:00",
            "updated": "2024-06-17 22:00:26+00:00"
        },
        {
            "title": "Decomposed evaluations of geographic disparities in text-to-image models",
            "authors": "Abhishek Sureddy, Dishant Padalia, Nandhinee Periyakaruppa, Oindrila Saha, Adina Williams, Adriana Romero-Soriano, Megan Richards, Polina Kirichenko, Melissa Hall",
            "summary": "Recent work has identified substantial disparities in generated images of\ndifferent geographic regions, including stereotypical depictions of everyday\nobjects like houses and cars. However, existing measures for these disparities\nhave been limited to either human evaluations, which are time-consuming and\ncostly, or automatic metrics evaluating full images, which are unable to\nattribute these disparities to specific parts of the generated images. In this\nwork, we introduce a new set of metrics, Decomposed Indicators of Disparities\nin Image Generation (Decomposed-DIG), that allows us to separately measure\ngeographic disparities in the depiction of objects and backgrounds in generated\nimages. Using Decomposed-DIG, we audit a widely used latent diffusion model and\nfind that generated images depict objects with better realism than backgrounds\nand that backgrounds in generated images tend to contain larger regional\ndisparities than objects. We use Decomposed-DIG to pinpoint specific examples\nof disparities, such as stereotypical background generation in Africa,\nstruggling to generate modern vehicles in Africa, and unrealistically placing\nsome objects in outdoor settings. Informed by our metric, we use a new\nprompting structure that enables a 52% worst-region improvement and a 20%\naverage improvement in generated background diversity.",
            "pdf_url": "http://arxiv.org/pdf/2406.11988v1",
            "published": "2024-06-17 18:04:23+00:00",
            "updated": "2024-06-17 18:04:23+00:00"
        },
        {
            "title": "Crossfusor: A Cross-Attention Transformer Enhanced Conditional Diffusion Model for Car-Following Trajectory Prediction",
            "authors": "Junwei You, Haotian Shi, Keshu Wu, Keke Long, Sicheng Fu, Sikai Chen, Bin Ran",
            "summary": "Vehicle trajectory prediction is crucial for advancing autonomous driving and\nadvanced driver assistance systems (ADAS), enhancing road safety and traffic\nefficiency. While traditional methods have laid foundational work, modern deep\nlearning techniques, particularly transformer-based models and generative\napproaches, have significantly improved prediction accuracy by capturing\ncomplex and non-linear patterns in vehicle motion and traffic interactions.\nHowever, these models often overlook the detailed car-following behaviors and\ninter-vehicle interactions essential for real-world driving scenarios. This\nstudy introduces a Cross-Attention Transformer Enhanced Conditional Diffusion\nModel (Crossfusor) specifically designed for car-following trajectory\nprediction. Crossfusor integrates detailed inter-vehicular interactions and\ncar-following dynamics into a robust diffusion framework, improving both the\naccuracy and realism of predicted trajectories. The model leverages a novel\ntemporal feature encoding framework combining GRU, location-based attention\nmechanisms, and Fourier embedding to capture historical vehicle dynamics. It\nemploys noise scaled by these encoded historical features in the forward\ndiffusion process, and uses a cross-attention transformer to model intricate\ninter-vehicle dependencies in the reverse denoising process. Experimental\nresults on the NGSIM dataset demonstrate that Crossfusor outperforms\nstate-of-the-art models, particularly in long-term predictions, showcasing its\npotential for enhancing the predictive capabilities of autonomous driving\nsystems.",
            "pdf_url": "http://arxiv.org/pdf/2406.11941v1",
            "published": "2024-06-17 17:35:47+00:00",
            "updated": "2024-06-17 17:35:47+00:00"
        },
        {
            "title": "Tracking the perspectives of interacting language models",
            "authors": "Hayden Helm, Brandon Duderstadt, Youngser Park, Carey E. Priebe",
            "summary": "Large language models (LLMs) are capable of producing high quality\ninformation at unprecedented rates. As these models continue to entrench\nthemselves in society, the content they produce will become increasingly\npervasive in databases that are, in turn, incorporated into the pre-training\ndata, fine-tuning data, retrieval data, etc. of other language models. In this\npaper we formalize the idea of a communication network of LLMs and introduce a\nmethod for representing the perspective of individual models within a\ncollection of LLMs. Given these tools we systematically study information\ndiffusion in the communication network of LLMs in various simulated settings.",
            "pdf_url": "http://arxiv.org/pdf/2406.11938v1",
            "published": "2024-06-17 17:20:16+00:00",
            "updated": "2024-06-17 17:20:16+00:00"
        }
    ],
    "Finance": [
        {
            "title": "Bridging Design Gaps: A Parametric Data Completion Approach With Graph Guided Diffusion Models",
            "authors": "Rui Zhou, Chenyang Yuan, Frank Permenter, Yanxia Zhang, Nikos Arechiga, Matt Klenk, Faez Ahmed",
            "summary": "This study introduces a generative imputation model leveraging graph\nattention networks and tabular diffusion models for completing missing\nparametric data in engineering designs. This model functions as an AI design\nco-pilot, providing multiple design options for incomplete designs, which we\ndemonstrate using the bicycle design CAD dataset. Through comparative\nevaluations, we demonstrate that our model significantly outperforms existing\nclassical methods, such as MissForest, hotDeck, PPCA, and tabular generative\nmethod TabCSDI in both the accuracy and diversity of imputation options.\nGenerative modeling also enables a broader exploration of design possibilities,\nthereby enhancing design decision-making by allowing engineers to explore a\nvariety of design completions. The graph model combines GNNs with the\nstructural information contained in assembly graphs, enabling the model to\nunderstand and predict the complex interdependencies between different design\nparameters. The graph model helps accurately capture and impute complex\nparametric interdependencies from an assembly graph, which is key for design\nproblems. By learning from an existing dataset of designs, the imputation\ncapability allows the model to act as an intelligent assistant that\nautocompletes CAD designs based on user-defined partial parametric design,\neffectively bridging the gap between ideation and realization. The proposed\nwork provides a pathway to not only facilitate informed design decisions but\nalso promote creative exploration in design.",
            "pdf_url": "http://arxiv.org/pdf/2406.11934v1",
            "published": "2024-06-17 16:03:17+00:00",
            "updated": "2024-06-17 16:03:17+00:00"
        },
        {
            "title": "Improving Quality Control of Whole Slide Images by Explicit Artifact Augmentation",
            "authors": "Artur Jurgas, Marek Wodzinski, Marina D'Amato, Jeroen van der Laak, Manfredo Atzori, Henning M\u00fcller",
            "summary": "The problem of artifacts in whole slide image acquisition, prevalent in both\nclinical workflows and research-oriented settings, necessitates human\nintervention and re-scanning. Overcoming this challenge requires developing\nquality control algorithms, that are hindered by the limited availability of\nrelevant annotated data in histopathology. The manual annotation of\nground-truth for artifact detection methods is expensive and time-consuming.\nThis work addresses the issue by proposing a method dedicated to augmenting\nwhole slide images with artifacts. The tool seamlessly generates and blends\nartifacts from an external library to a given histopathology dataset. The\naugmented datasets are then utilized to train artifact classification methods.\nThe evaluation shows their usefulness in classification of the artifacts, where\nthey show an improvement from 0.10 to 0.01 AUROC depending on the artifact\ntype. The framework, model, weights, and ground-truth annotations are freely\nreleased to facilitate open science and reproducible research.",
            "pdf_url": "http://arxiv.org/pdf/2406.11538v1",
            "published": "2024-06-17 13:39:31+00:00",
            "updated": "2024-06-17 13:39:31+00:00"
        },
        {
            "title": "Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms",
            "authors": "Vaneet Aggarwal, Washim Uddin Mondal, Qinbo Bai",
            "summary": "Reinforcement Learning (RL) serves as a versatile framework for sequential\ndecision-making, finding applications across diverse domains such as robotics,\nautonomous driving, recommendation systems, supply chain optimization, biology,\nmechanics, and finance. The primary objective in these applications is to\nmaximize the average reward. Real-world scenarios often necessitate adherence\nto specific constraints during the learning process.\n  This monograph focuses on the exploration of various model-based and\nmodel-free approaches for Constrained RL within the context of average reward\nMarkov Decision Processes (MDPs). The investigation commences with an\nexamination of model-based strategies, delving into two foundational methods -\noptimism in the face of uncertainty and posterior sampling. Subsequently, the\ndiscussion transitions to parametrized model-free approaches, where the\nprimal-dual policy gradient-based algorithm is explored as a solution for\nconstrained MDPs. The monograph provides regret guarantees and analyzes\nconstraint violation for each of the discussed setups.\n  For the above exploration, we assume the underlying MDP to be ergodic.\nFurther, this monograph extends its discussion to encompass results tailored\nfor weakly communicating MDPs, thereby broadening the scope of its findings and\ntheir relevance to a wider range of practical scenarios.",
            "pdf_url": "http://arxiv.org/pdf/2406.11481v1",
            "published": "2024-06-17 12:46:02+00:00",
            "updated": "2024-06-17 12:46:02+00:00"
        }
    ]
}