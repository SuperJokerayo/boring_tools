{
    "Physics": [
        {
            "title": "Transverse Energy-Energy Correlator for Vector Boson-Tagged Hadron Production in $pp$ and $pA$ collisions",
            "authors": "Zhong-Bo Kang, Sookhyun Lee, Jani Penttala, Fanyi Zhao, Yiyu Zhou",
            "summary": "We investigate the transverse energy-energy correlator (TEEC) event-shape\nobservable for back-to-back $\\gamma + h$ and $Z + h$ production in both $pp$\nand $pA$ collisions. Our study incorporates nuclear modifications into the\ntransverse-momentum dependent (TMD) factorization framework, with resummation\nup to next-to-leading logarithmic (NLL) accuracy, for TEEC as a function of the\nvariable $\\tau = \\left(1 + \\cos{\\phi} \\right)/2$, where $\\phi$ is the azimuthal\nangle between the vector boson and the final hadron. We analyze the nuclear\nmodification factor $R_{pA}$ in $p\\mathrm{Au}$ collisions at RHIC and\n$p\\mathrm{Pb}$ collisions at the LHC. Our results demonstrate that the TEEC\nobservable is a sensitive probe for nuclear modifications in TMD physics.\nSpecifically, the changes in the $\\tau$-distribution shape provide insights\ninto transverse momentum broadening effects in large nuclei, while measurements\nat different rapidities allow us to explore nuclear modifications in the\ncollinear component of the TMD parton distribution functions in nuclei.",
            "pdf_url": "http://arxiv.org/pdf/2410.02747v1",
            "published": "2024-10-03 17:56:57+00:00",
            "updated": "2024-10-03 17:56:57+00:00"
        },
        {
            "title": "Grounding Large Language Models In Embodied Environment With Imperfect World Models",
            "authors": "Haolan Liu, Jishen Zhao",
            "summary": "Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.",
            "pdf_url": "http://arxiv.org/pdf/2410.02742v1",
            "published": "2024-10-03 17:55:09+00:00",
            "updated": "2024-10-03 17:55:09+00:00"
        },
        {
            "title": "Duality between string and computational order in symmetry-enriched topological phases",
            "authors": "Paul Herringer, Vir B. Bulchandani, Younes Javanmard, David T. Stephen, Robert Raussendorf",
            "summary": "We present the first examples of topological phases of matter with uniform\npower for measurement-based quantum computation. This is possible thanks to a\nnew framework for analyzing the computational properties of phases of matter\nthat is more general than previous constructions, which were limited to\nshort-range entangled phases in one dimension. We show that ground states of\nthe toric code in an anisotropic magnetic field yield a natural, albeit\nnon-computationally-universal, application of our framework. We then present a\nnew model with topological order whose ground states are universal resources\nfor MBQC. Both topological models are enriched by subsystem symmetries, and\nthese symmetries protect their computational power. Our framework greatly\nexpands the range of physical models that can be analyzed from the\ncomputational perspective.",
            "pdf_url": "http://arxiv.org/pdf/2410.02716v1",
            "published": "2024-10-03 17:38:03+00:00",
            "updated": "2024-10-03 17:38:03+00:00"
        },
        {
            "title": "VideoPhy: Evaluating Physical Commonsense for Video Generation",
            "authors": "Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, Aditya Grover",
            "summary": "Recent advances in internet-scale video data pretraining have led to the\ndevelopment of text-to-video generative models that can create high-quality\nvideos across a broad range of visual concepts, synthesize realistic motions\nand render complex objects. Hence, these generative models have the potential\nto become general-purpose simulators of the physical world. However, it is\nunclear how far we are from this goal with the existing text-to-video\ngenerative models. To this end, we present VideoPhy, a benchmark designed to\nassess whether the generated videos follow physical commonsense for real-world\nactivities (e.g. marbles will roll down when placed on a slanted surface).\nSpecifically, we curate diverse prompts that involve interactions between\nvarious material types in the physical world (e.g., solid-solid, solid-fluid,\nfluid-fluid). We then generate videos conditioned on these captions from\ndiverse state-of-the-art text-to-video generative models, including open models\n(e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human\nevaluation reveals that the existing models severely lack the ability to\ngenerate videos adhering to the given text prompts, while also lack physical\ncommonsense. Specifically, the best performing model, CogVideoX-5B, generates\nvideos that adhere to the caption and physical laws for 39.6% of the instances.\nVideoPhy thus highlights that the video generative models are far from\naccurately simulating the physical world. Finally, we propose an\nauto-evaluator, VideoCon-Physics, to assess the performance reliably for the\nnewly released models.",
            "pdf_url": "http://arxiv.org/pdf/2406.03520v2",
            "published": "2024-06-05 17:53:55+00:00",
            "updated": "2024-10-03 17:24:40+00:00"
        },
        {
            "title": "Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups",
            "authors": "Zakhar Shumaylov, Peter Zaika, James Rowbottom, Ferdia Sherry, Melanie Weber, Carola-Bibiane Sch\u00f6nlieb",
            "summary": "The quest for robust and generalizable machine learning models has driven\nrecent interest in exploiting symmetries through equivariant neural networks.\nIn the context of PDE solvers, recent works have shown that Lie point\nsymmetries can be a useful inductive bias for Physics-Informed Neural Networks\n(PINNs) through data and loss augmentation. Despite this, directly enforcing\nequivariance within the model architecture for these problems remains elusive.\nThis is because many PDEs admit non-compact symmetry groups, oftentimes not\nstudied beyond their infinitesimal generators, making them incompatible with\nmost existing equivariant architectures. In this work, we propose Lie aLgebrA\nCanonicalization (LieLAC), a novel approach that exploits only the action of\ninfinitesimal generators of the symmetry group, circumventing the need for\nknowledge of the full group structure. To achieve this, we address existing\ntheoretical issues in the canonicalization literature, establishing connections\nwith frame averaging in the case of continuous non-compact groups. Operating\nwithin the framework of canonicalization, LieLAC can easily be integrated with\nunconstrained pre-trained models, transforming inputs to a canonical form\nbefore feeding them into the existing model, effectively aligning the input for\nmodel inference according to allowed symmetries. LieLAC utilizes standard Lie\ngroup descent schemes, achieving equivariance in pre-trained models. Finally,\nwe showcase LieLAC's efficacy on tasks of invariant image classification and\nLie point symmetry equivariant neural PDE solvers using pre-trained models.",
            "pdf_url": "http://arxiv.org/pdf/2410.02698v1",
            "published": "2024-10-03 17:21:30+00:00",
            "updated": "2024-10-03 17:21:30+00:00"
        },
        {
            "title": "Prospects of phase-adaptive cooling of levitated magnetic particles in a hollow-core photonic-crystal fibre",
            "authors": "P. Kumar, F. G. Jimenez, S. Chakraborty, G. K. L. Wong, N. Y. Joly, C. Genes",
            "summary": "We analyze the feasibility of cooling of classical motion of a micro- to\nnano-sized magnetic particle, levitated inside a hollow-core photonic crystal\nfiber. The cooling action is implemented by means of controlling the phase of\none of the counter-propagating fiber guided waves. Direct imaging of the\nparticle's position, followed by the subsequent updating of the control laser's\nphase leads to Stokes type of cooling force. We provide estimates of cooling\nefficiency and final achievable temperature, taking into account thermal and\ndetection noise sources. Our results bring forward an important step towards\nusing trapped micro-magnets in sensing, testing the fundamental physics and\npreparing the quantum states of magnetization.",
            "pdf_url": "http://arxiv.org/pdf/2410.02697v1",
            "published": "2024-10-03 17:20:49+00:00",
            "updated": "2024-10-03 17:20:49+00:00"
        },
        {
            "title": "Supergeometric Quantum Effective Action",
            "authors": "Viola Gattus, Apostolos Pilaftsis",
            "summary": "Supergeometric Quantum Field Theories (SG-QFTs) are theories that go beyond\nthe standard supersymmetric framework, since they allow for general\nscalar-fermion field transformations on the configuration space of a\nsupermanifold, without requiring an equality between bosonic and fermionic\ndegrees of freedom. After revisiting previous considerations, we extend them by\ncalculating the one-loop effective action of minimal SG-QFTs that feature\nnon-zero fermionic curvature in two and four spacetime dimensions. By employing\nan intuitive approach to the Schwinger-DeWitt heat-kernel technique and a novel\nfield-space generalised Clifford algebra, we derive the ultra-violet structure\nof characteristic effective-field-theory (EFT) operators up to four spacetime\nderivatives that emerge at the one-loop order and are of physical interest.\nUpon minimising the impact of potential ambiguities due to the so-called\nmultiplicative anomalies, we find that the EFT interactions resulting from the\none-loop supergeometric effective action are manifestly diffeomorphically\ninvariant in configuration space. The extension of our approach to evaluating\nhigher-loops of the supergeometric quantum effective action is described. The\nemerging landscape of theoretical and phenomenological directions for further\nresearch of SG-QFTs is discussed.",
            "pdf_url": "http://arxiv.org/pdf/2406.13594v2",
            "published": "2024-06-19 14:33:44+00:00",
            "updated": "2024-10-03 16:51:52+00:00"
        },
        {
            "title": "GUD: Generation with Unified Diffusion",
            "authors": "Mathis Gerdes, Max Welling, Miranda C. N. Cheng",
            "summary": "Diffusion generative models transform noise into data by inverting a process\nthat progressively adds noise to data samples. Inspired by concepts from the\nrenormalization group in physics, which analyzes systems across different\nscales, we revisit diffusion models by exploring three key design aspects: 1)\nthe choice of representation in which the diffusion process operates (e.g.\npixel-, PCA-, Fourier-, or wavelet-basis), 2) the prior distribution that data\nis transformed into during diffusion (e.g. Gaussian with covariance $\\Sigma$),\nand 3) the scheduling of noise levels applied separately to different parts of\nthe data, captured by a component-wise noise schedule. Incorporating the\nflexibility in these choices, we develop a unified framework for diffusion\ngenerative models with greatly enhanced design freedom. In particular, we\nintroduce soft-conditioning models that smoothly interpolate between standard\ndiffusion models and autoregressive models (in any basis), conceptually\nbridging these two approaches. Our framework opens up a wide design space which\nmay lead to more efficient training and data generation, and paves the way to\nnovel architectures integrating different generative approaches and generation\ntasks.",
            "pdf_url": "http://arxiv.org/pdf/2410.02667v1",
            "published": "2024-10-03 16:51:14+00:00",
            "updated": "2024-10-03 16:51:14+00:00"
        },
        {
            "title": "CAX: Cellular Automata Accelerated in JAX",
            "authors": "Maxence Faldor, Antoine Cully",
            "summary": "Cellular automata have become a cornerstone for investigating emergence and\nself-organization across diverse scientific disciplines, spanning neuroscience,\nartificial life, and theoretical physics. However, the absence of a\nhardware-accelerated cellular automata library limits the exploration of new\nresearch directions, hinders collaboration, and impedes reproducibility. In\nthis work, we introduce CAX (Cellular Automata Accelerated in JAX), a\nhigh-performance and flexible open-source library designed to accelerate\ncellular automata research. CAX offers cutting-edge performance and a modular\ndesign through a user-friendly interface, and can support both discrete and\ncontinuous cellular automata with any number of dimensions. We demonstrate\nCAX's performance and flexibility through a wide range of benchmarks and\napplications. From classic models like elementary cellular automata and\nConway's Game of Life to advanced applications such as growing neural cellular\nautomata and self-classifying MNIST digits, CAX speeds up simulations up to\n2,000 times faster. Furthermore, we demonstrate CAX's potential to accelerate\nresearch by presenting a collection of three novel cellular automata\nexperiments, each implemented in just a few lines of code thanks to the\nlibrary's modular architecture. Notably, we show that a simple one-dimensional\ncellular automaton can outperform GPT-4 on the 1D-ARC challenge.",
            "pdf_url": "http://arxiv.org/pdf/2410.02651v1",
            "published": "2024-10-03 16:36:05+00:00",
            "updated": "2024-10-03 16:36:05+00:00"
        },
        {
            "title": "Quantum many-body solver using artificial neural networks and its applications to strongly correlated electron systems",
            "authors": "Yusuke Nomura, Masatoshi Imada",
            "summary": "With the evolution of numerical methods, we are now aiming at not only\nqualitative understanding but also quantitative prediction and design of\nquantum many-body phenomena. As a novel numerical approach, machine learning\ntechniques have been introduced in 2017 to analyze quantum many-body problems.\nSince then, proposed various novel approaches have opened a new era, in which\nchallenging and fundamental problems in physics can be solved by machine\nlearning methods. Especially, quantitative and accurate estimates of\nmaterial-dependent physical properties of strongly correlated matter have now\nbecome realized by combining first-principles calculations with highly accurate\nquantum many-body solvers developed with the help of machine learning methods.\nThus developed quantitative description of electron correlations will\nconstitute a key element of materials science in the next generation.",
            "pdf_url": "http://arxiv.org/pdf/2410.02633v1",
            "published": "2024-10-03 16:16:26+00:00",
            "updated": "2024-10-03 16:16:26+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models",
            "authors": "Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch, Meng Cao, Yinfei Yang",
            "summary": "Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models.",
            "pdf_url": "http://arxiv.org/pdf/2410.02740v1",
            "published": "2024-10-03 17:54:52+00:00",
            "updated": "2024-10-03 17:54:52+00:00"
        },
        {
            "title": "NETS: A Non-Equilibrium Transport Sampler",
            "authors": "Michael S. Albergo, Eric Vanden-Eijnden",
            "summary": "We propose an algorithm, termed the Non-Equilibrium Transport Sampler (NETS),\nto sample from unnormalized probability distributions. NETS can be viewed as a\nvariant of annealed importance sampling (AIS) based on Jarzynski's equality, in\nwhich the stochastic differential equation used to perform the non-equilibrium\nsampling is augmented with an additional learned drift term that lowers the\nimpact of the unbiasing weights used in AIS. We show that this drift is the\nminimizer of a variety of objective functions, which can all be estimated in an\nunbiased fashion without backpropagating through solutions of the stochastic\ndifferential equations governing the sampling. We also prove that some these\nobjectives control the Kullback-Leibler divergence of the estimated\ndistribution from its target. NETS is shown to be unbiased and, in addition,\nhas a tunable diffusion coefficient which can be adjusted post-training to\nmaximize the effective sample size. We demonstrate the efficacy of the method\non standard benchmarks, high-dimensional Gaussian mixture distributions, and a\nmodel from statistical lattice field theory, for which it surpasses the\nperformances of related work and existing baselines.",
            "pdf_url": "http://arxiv.org/pdf/2410.02711v1",
            "published": "2024-10-03 17:35:38+00:00",
            "updated": "2024-10-03 17:35:38+00:00"
        },
        {
            "title": "SteerDiff: Steering towards Safe Text-to-Image Diffusion Models",
            "authors": "Hongxiang Zhang, Yifeng He, Hao Chen",
            "summary": "Text-to-image (T2I) diffusion models have drawn attention for their ability\nto generate high-quality images with precise text alignment. However, these\nmodels can also be misused to produce inappropriate content. Existing safety\nmeasures, which typically rely on text classifiers or ControlNet-like\napproaches, are often insufficient. Traditional text classifiers rely on\nlarge-scale labeled datasets and can be easily bypassed by rephrasing. As\ndiffusion models continue to scale, fine-tuning these safeguards becomes\nincreasingly challenging and lacks flexibility. Recent red-teaming attack\nresearches further underscore the need for a new paradigm to prevent the\ngeneration of inappropriate content. In this paper, we introduce SteerDiff, a\nlightweight adaptor module designed to act as an intermediary between user\ninput and the diffusion model, ensuring that generated images adhere to ethical\nand safety standards with little to no impact on usability. SteerDiff\nidentifies and manipulates inappropriate concepts within the text embedding\nspace to guide the model away from harmful outputs. We conduct extensive\nexperiments across various concept unlearning tasks to evaluate the\neffectiveness of our approach. Furthermore, we benchmark SteerDiff against\nmultiple red-teaming strategies to assess its robustness. Finally, we explore\nthe potential of SteerDiff for concept forgetting tasks, demonstrating its\nversatility in text-conditioned image generation.",
            "pdf_url": "http://arxiv.org/pdf/2410.02710v1",
            "published": "2024-10-03 17:34:55+00:00",
            "updated": "2024-10-03 17:34:55+00:00"
        },
        {
            "title": "Undesirable Memorization in Large Language Models: A Survey",
            "authors": "Ali Satvaty, Suzan Verberne, Fatih Turkmen",
            "summary": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models.",
            "pdf_url": "http://arxiv.org/pdf/2410.02650v1",
            "published": "2024-10-03 16:34:46+00:00",
            "updated": "2024-10-03 16:34:46+00:00"
        },
        {
            "title": "NECOMIMI: Neural-Cognitive Multimodal EEG-informed Image Generation with Diffusion Models",
            "authors": "Chi-Sheng Chen",
            "summary": "NECOMIMI (NEural-COgnitive MultImodal EEG-Informed Image Generation with\nDiffusion Models) introduces a novel framework for generating images directly\nfrom EEG signals using advanced diffusion models. Unlike previous works that\nfocused solely on EEG-image classification through contrastive learning,\nNECOMIMI extends this task to image generation. The proposed NERV EEG encoder\ndemonstrates state-of-the-art (SoTA) performance across multiple zero-shot\nclassification tasks, including 2-way, 4-way, and 200-way, and achieves top\nresults in our newly proposed Category-based Assessment Table (CAT) Score,\nwhich evaluates the quality of EEG-generated images based on semantic concepts.\nA key discovery of this work is that the model tends to generate abstract or\ngeneralized images, such as landscapes, rather than specific objects,\nhighlighting the inherent challenges of translating noisy and low-resolution\nEEG data into detailed visual outputs. Additionally, we introduce the CAT Score\nas a new metric tailored for EEG-to-image evaluation and establish a benchmark\non the ThingsEEG dataset. This study underscores the potential of EEG-to-image\ngeneration while revealing the complexities and challenges that remain in\nbridging neural activity with visual representation.",
            "pdf_url": "http://arxiv.org/pdf/2410.00712v2",
            "published": "2024-10-01 14:05:30+00:00",
            "updated": "2024-10-03 16:31:23+00:00"
        }
    ]
}