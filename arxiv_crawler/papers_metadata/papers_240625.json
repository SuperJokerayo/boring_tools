{
    "Physics": [
        {
            "title": "GeoMFormer: A General Architecture for Geometric Molecular Representation Learning",
            "authors": "Tianlang Chen, Shengjie Luo, Di He, Shuxin Zheng, Tie-Yan Liu, Liwei Wang",
            "summary": "Molecular modeling, a central topic in quantum mechanics, aims to accurately\ncalculate the properties and simulate the behaviors of molecular systems. The\nmolecular model is governed by physical laws, which impose geometric\nconstraints such as invariance and equivariance to coordinate rotation and\ntranslation. While numerous deep learning approaches have been developed to\nlearn molecular representations under these constraints, most of them are built\nupon heuristic and costly modules. We argue that there is a strong need for a\ngeneral and flexible framework for learning both invariant and equivariant\nfeatures. In this work, we introduce a novel Transformer-based molecular model\ncalled GeoMFormer to achieve this goal. Using the standard Transformer modules,\ntwo separate streams are developed to maintain and learn invariant and\nequivariant representations. Carefully designed cross-attention modules bridge\nthe two streams, allowing information fusion and enhancing geometric modeling\nin each stream. As a general and flexible architecture, we show that many\nprevious architectures can be viewed as special instantiations of GeoMFormer.\nExtensive experiments are conducted to demonstrate the power of GeoMFormer. All\nempirical results show that GeoMFormer achieves strong performance on both\ninvariant and equivariant tasks of different types and scales. Code and models\nwill be made publicly available at https://github.com/c-tl/GeoMFormer.",
            "pdf_url": "http://arxiv.org/pdf/2406.16853v1",
            "published": "2024-06-24 17:58:13+00:00",
            "updated": "2024-06-24 17:58:13+00:00"
        },
        {
            "title": "Improving physics-informed DeepONets with hard constraints",
            "authors": "R\u00fcdiger Brecht, Dmytro R. Popovych, Alex Bihlo, Roman O. Popovych",
            "summary": "Current physics-informed (standard or deep operator) neural networks still\nrely on accurately learning the initial and/or boundary conditions of the\nsystem of differential equations they are solving. In contrast, standard\nnumerical methods involve such conditions in computations without needing to\nlearn them. In this study, we propose to improve current physics-informed deep\nlearning strategies such that initial and/or boundary conditions do not need to\nbe learned and are represented exactly in the predicted solution. Moreover,\nthis method guarantees that when a deep operator network is applied multiple\ntimes to time-step a solution of an initial value problem, the resulting\nfunction is at least continuous.",
            "pdf_url": "http://arxiv.org/pdf/2309.07899v2",
            "published": "2023-09-14 17:48:30+00:00",
            "updated": "2024-06-24 17:54:58+00:00"
        },
        {
            "title": "Realizing a spatially correlated lattice interferometer",
            "authors": "Peng Peng, Dekai Mao, Yi Liang, Guoling Yin, Hongmian Shui, Bo Song, Xiaoji Zhou",
            "summary": "Atom interferometers provide a powerful tool for measuring physical constants\nand testifying fundamental physics with unprecedented precision. Conventional\natom interferometry focuses on the phase difference between two paths and\nutilizes matter waves with fixed coherence. Here, we report on realizing a\nRamsey-Bord\\'e interferometer of coherent matter waves dressed by a moving\noptical lattice in the gravity direction, and explore the resulting\ninterference along multiple paths with tunable coherence. We investigate\nspatial correlations of atoms both within the lattice and between two arms by\ninterferometry, and observe the emerging multiple interference peaks owing to\nthe long-range coherence nature of the Bose-Einstein condensate. Our findings\nagree well with theoretical simulations, paving the way for high-precision\ninterferometry with ultracold atoms.",
            "pdf_url": "http://arxiv.org/pdf/2406.16847v1",
            "published": "2024-06-24 17:54:03+00:00",
            "updated": "2024-06-24 17:54:03+00:00"
        },
        {
            "title": "Scaling and renormalization in high-dimensional regression",
            "authors": "Alexander Atanasov, Jacob A. Zavatone-Veth, Cengiz Pehlevan",
            "summary": "This paper presents a succinct derivation of the training and generalization\nperformance of a variety of high-dimensional ridge regression models using the\nbasic tools of random matrix theory and free probability. We provide an\nintroduction and review of recent results on these topics, aimed at readers\nwith backgrounds in physics and deep learning. Analytic formulas for the\ntraining and generalization errors are obtained in a few lines of algebra\ndirectly from the properties of the $S$-transform of free probability. This\nallows for a straightforward identification of the sources of power-law scaling\nin model performance. We compute the generalization error of a broad class of\nrandom feature models. We find that in all models, the $S$-transform\ncorresponds to the train-test generalization gap, and yields an analogue of the\ngeneralized-cross-validation estimator. Using these techniques, we derive\nfine-grained bias-variance decompositions for a very general class of random\nfeature models with structured covariates. These novel results allow us to\ndiscover a scaling regime for random feature models where the variance due to\nthe features limits performance in the overparameterized setting. We also\ndemonstrate how anisotropic weight structure in random feature models can limit\nperformance and lead to nontrivial exponents for finite-width corrections in\nthe overparameterized setting. Our results extend and provide a unifying\nperspective on earlier models of neural scaling laws.",
            "pdf_url": "http://arxiv.org/pdf/2405.00592v2",
            "published": "2024-05-01 15:59:00+00:00",
            "updated": "2024-06-24 17:47:41+00:00"
        },
        {
            "title": "Damping effects of viscous dissipation on growth of symmetric instability",
            "authors": "Laur Ferris, Donglai Gong",
            "summary": "Symmetric instability (SI) is a frontal instability arising from the\ninteraction of rotation with lateral and vertical shear of a frontal jet and is\na generalization of shear, centrifugal, and gravitational instabilities. While\nthe onset of SI has been studied in numerous observations and models, intuition\nabout its growth in physical ocean comes primarily from constant-viscosity\nlinear instability analysis and large eddy simulation (LES). A forward cascade\narising from SI in the real ocean, where numerous fine-to-microscale processes\ninteract with growing SI velocity cells, is less understood. While many\ninstances of symmetrically unstable flow have been observed, observations of\nenhanced turbulent kinetic energy (TKE) dissipation ($\\epsilon$) at these sites\nare less common. We use numerical instability analysis of an idealized\ngeostrophic jet to show that viscous-diffusive effects of preexisting\nturbulence from other turbulent processes (e.g., competing instabilities,\ninternal wave processes, or boundary layer processes) can suppress the growth\nof SI in the real ocean. For example, a moderate level of ambient turbulence,\nrepresented by uniform diffusivity and viscosity of $\\kappa =\\nu = 10^{-4}\nm^2/s$, restricts the wavelength range of SI's fastest-growing mode from\n$\\mathcal{O}(10-100)$m to $\\mathcal{O}(100)$m and elongates its e-folding\ntimescale by $\\mathcal{O}(1-10)$ hrs; suggesting the net viscous-diffusive\neffects of preexisting turbulence can damp the growth of SI. Viscous damping is\none possible explanation for the rarity of SI structures in the real ocean, and\nour results motivate the inclusion of dependence on previous-timestep\n$\\epsilon$ or $\\kappa$ when parameterizing SI in regional models.",
            "pdf_url": "http://arxiv.org/pdf/2406.16818v1",
            "published": "2024-06-24 17:27:06+00:00",
            "updated": "2024-06-24 17:27:06+00:00"
        },
        {
            "title": "Discovering neutrino tridents at the Large Hadron Collider",
            "authors": "Wolfgang Altmannshofer, Toni M\u00e4kel\u00e4, Subir Sarkar, Sebastian Trojanowski, Keping Xie, Bei Zhou",
            "summary": "Neutrino trident production of di-lepton pairs is well recognized as a\nsensitive probe of both electroweak physics and physics beyond the Standard\nModel. Although a rare process, it could be significantly boosted by such new\nphysics, and it also allows the electroweak theory to be tested in a new\nregime. We demonstrate that the forward neutrino physics program at the Large\nHadron Collider offers a promising opportunity to measure for the first time,\ndimuon neutrino tridents with a statistical significance exceeding $5\\sigma$.\nWe present predictions for various proposed experiments and outline a specific\nexperimental strategy to identify the signal and mitigate backgrounds, based on\n\"reverse tracking\" dimuon pairs in the FASER$\\nu$2 detector. We also discuss\nprospects for constraining beyond Standard Model contributions to neutrino\ntrident rates at high energies.",
            "pdf_url": "http://arxiv.org/pdf/2406.16803v1",
            "published": "2024-06-24 17:14:31+00:00",
            "updated": "2024-06-24 17:14:31+00:00"
        },
        {
            "title": "WISER: multimodal variational inference for full-waveform inversion without dimensionality reduction",
            "authors": "Ziyi Yin, Rafael Orozco, Felix J. Herrmann",
            "summary": "We present a semi-amortized variational inference framework designed for\ncomputationally feasible uncertainty quantification in 2D full-waveform\ninversion to explore the multimodal posterior distribution without\ndimensionality reduction. The framework is called WISER, short for\nfull-Waveform variational Inference via Subsurface Extensions with Refinements.\nWISER leverages the power of generative artificial intelligence to perform\napproximate amortized inference that is low-cost albeit showing an amortization\ngap. This gap is closed through non-amortized refinements that make frugal use\nof acoustic wave physics. Case studies illustrate that WISER is capable of\nfull-resolution, computationally feasible, and reliable uncertainty estimates\nof velocity models and imaged reflectivities.",
            "pdf_url": "http://arxiv.org/pdf/2405.10327v2",
            "published": "2024-05-03 19:35:16+00:00",
            "updated": "2024-06-24 17:08:09+00:00"
        },
        {
            "title": "A Mereological Approach to Higher-Order Structure in Complex Systems: from Macro to Micro with M\u00f6bius",
            "authors": "Abel Jansma",
            "summary": "Relating microscopic interactions to macroscopic observables is a central\nchallenge in the study of complex systems. Addressing this question requires\nunderstanding both pairwise and \\textit{higher-order} interactions, but the\nlatter are less well understood. Here, we show that the M\\\"obius inversion\ntheorem provides a general mathematical formalism for deriving higher-order\ninteractions from macroscopic observables, relative to a chosen decomposition\nof the system into parts. Applying this framework to a diverse range of\nsystems, we demonstrate that many existing notions of higher-order\ninteractions, from epistasis in genetics and many-body couplings in physics, to\nsynergy in game theory and artificial intelligence, naturally and uniquely\narise from an appropriate mereological decomposition. By revealing the common\nmathematical structure underlying seemingly disparate phenomena, our work\nhighlights the fundamental role of decomposition choice in the definition and\nestimation of higher-order interactions. We discuss how this unifying\nperspective can facilitate the transfer of insights between domains, guide the\nselection of appropriate system decompositions, and motivate the search for\nnovel interaction types through new decomposition strategies. To illustrate how\nthis works in practice, we derive a new decomposition of the KL-divergence, and\nshow that it correctly disentangles divergences at different orders on\nsimulated spin models. Our results suggest that the M\\\"obius inversion theorem\nprovides a powerful and practical lens for understanding the emergence of\ncomplex behaviour from the interplay of microscopic parts, with applications\nacross a wide range of disciplines.",
            "pdf_url": "http://arxiv.org/pdf/2404.14423v3",
            "published": "2024-04-17 09:40:54+00:00",
            "updated": "2024-06-24 16:57:37+00:00"
        },
        {
            "title": "Quantum resolution limit of long-baseline imaging using distributed entanglement",
            "authors": "Isack Padilla, Aqil Sajjad, Babak N. Saif, Saikat Guha",
            "summary": "It has been shown that shared entanglement between two telescope sites can in\nprinciple be used to localize a point source by mimicking the standard\nphase-scanning interferometer, but without physically bringing the light from\nthe distant telescopes together. In this paper, we show that a receiver that\nemploys spatial-mode sorting at each telescope site, combined with pre-shared\nentanglement and local quantum operations can be used to mimic the most general\nmultimode interferometer acting on light collected from the telescopes. As an\nexample application to a quantitative passive-imaging problem, we show that the\nquantum-limited precision of estimating the angular separation between two\nstars can be attained by an instantiation of the aforesaid entanglement based\nreceiver. We discuss how this entanglement assisted strategy can be used to\nachieve the quantum-limited precision of any complex quantitative imaging task\ninvolving any number of telescopes. We provide a blueprint of this general\nreceiver that involves quantum transduction of starlight into quantum memory\nbanks and spatial mode sorters deployed at each telescope site, and\nmeasurements that include optical detection as well as qubit gates and\nmeasurements on the quantum memories. We discuss the relative contributions of\nlocal mode sorting at telescope sites vis-a-vis distributed\nentanglement-assisted interferometry, to the overall quantum-limited\ninformation about the scene, based on the ratio of the baseline distance to the\nindividual telescope diameter.",
            "pdf_url": "http://arxiv.org/pdf/2406.16789v1",
            "published": "2024-06-24 16:50:10+00:00",
            "updated": "2024-06-24 16:50:10+00:00"
        },
        {
            "title": "OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?",
            "authors": "Zhen Huang, Zengzhi Wang, Shijie Xia, Pengfei Liu",
            "summary": "In this report, we pose the following question: Who is the most intelligent\nAI model to date, as measured by the OlympicArena (an Olympic-level,\nmulti-discipline, multi-modal benchmark for superintelligent AI)? We\nspecifically focus on the most recently released models: Claude-3.5-Sonnet,\nGemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic\nmedal Table approach to rank AI models based on their comprehensive performance\nacross various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet\nshows highly competitive overall performance over GPT-4o, even surpassing\nGPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)\nGemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and\nClaude-3.5-Sonnet, but with a clear performance gap between them. (3) The\nperformance of AI models from the open-source community significantly lags\nbehind these proprietary models. (4) The performance of these models on this\nbenchmark has been less than satisfactory, indicating that we still have a long\nway to go before achieving superintelligence. We remain committed to\ncontinuously tracking and evaluating the performance of the latest powerful\nmodels on this benchmark (available at\nhttps://github.com/GAIR-NLP/OlympicArena).",
            "pdf_url": "http://arxiv.org/pdf/2406.16772v1",
            "published": "2024-06-24 16:31:12+00:00",
            "updated": "2024-06-24 16:31:12+00:00"
        }
    ],
    "Diffusion": [
        {
            "title": "StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal",
            "authors": "Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, Xiaoguang Han",
            "summary": "This work addresses the challenge of high-quality surface normal estimation\nfrom monocular colored inputs (i.e., images and videos), a field which has\nrecently been revolutionized by repurposing diffusion priors. However, previous\nattempts still struggle with stochastic inference, conflicting with the\ndeterministic nature of the Image2Normal task, and costly ensembling step,\nwhich slows down the estimation process. Our method, StableNormal, mitigates\nthe stochasticity of the diffusion process by reducing inference variance, thus\nproducing \"Stable-and-Sharp\" normal estimates without any additional ensembling\nprocess. StableNormal works robustly under challenging imaging conditions, such\nas extreme lighting, blurring, and low quality. It is also robust against\ntransparent and reflective surfaces, as well as cluttered scenes with numerous\nobjects. Specifically, StableNormal employs a coarse-to-fine strategy, which\nstarts with a one-step normal estimator (YOSO) to derive an initial normal\nguess, that is relatively coarse but reliable, then followed by a\nsemantic-guided refinement process (SG-DRN) that refines the normals to recover\ngeometric details. The effectiveness of StableNormal is demonstrated through\ncompetitive performance in standard datasets such as DIODE-indoor, iBims,\nScannetV2 and NYUv2, and also in various downstream tasks, such as surface\nreconstruction and normal enhancement. These results evidence that StableNormal\nretains both the \"stability\" and \"sharpness\" for accurate normal estimation.\nStableNormal represents a baby attempt to repurpose diffusion priors for\ndeterministic estimation. To democratize this, code and models have been\npublicly available in hf.co/Stable-X",
            "pdf_url": "http://arxiv.org/pdf/2406.16864v1",
            "published": "2024-06-24 17:59:58+00:00",
            "updated": "2024-06-24 17:59:58+00:00"
        },
        {
            "title": "General Binding Affinity Guidance for Diffusion Models in Structure-Based Drug Design",
            "authors": "Yue Jian, Curtis Wu, Danny Reidenbach, Aditi S. Krishnapriyan",
            "summary": "Structure-Based Drug Design (SBDD) focuses on generating valid ligands that\nstrongly and specifically bind to a designated protein pocket. Several methods\nuse machine learning for SBDD to generate these ligands in 3D space,\nconditioned on the structure of a desired protein pocket. Recently, diffusion\nmodels have shown success here by modeling the underlying distributions of\natomic positions and types. While these methods are effective in considering\nthe structural details of the protein pocket, they often fail to explicitly\nconsider the binding affinity. Binding affinity characterizes how tightly the\nligand binds to the protein pocket, and is measured by the change in free\nenergy associated with the binding process. It is one of the most crucial\nmetrics for benchmarking the effectiveness of the interaction between a ligand\nand protein pocket. To address this, we propose BADGER: Binding Affinity\nDiffusion Guidance with Enhanced Refinement. BADGER is a general guidance\nmethod to steer the diffusion sampling process towards improved protein-ligand\nbinding, allowing us to adjust the distribution of the binding affinity between\nligands and proteins. Our method is enabled by using a neural network (NN) to\nmodel the energy function, which is commonly approximated by AutoDock Vina\n(ADV). ADV's energy function is non-differentiable, and estimates the affinity\nbased on the interactions between a ligand and target protein receptor. By\nusing a NN as a differentiable energy function proxy, we utilize the gradient\nof our learned energy function as a guidance method on top of any trained\ndiffusion model. We show that our method improves the binding affinity of\ngenerated ligands to their protein receptors by up to 60\\%, significantly\nsurpassing previous machine learning methods. We also show that our guidance\nmethod is flexible and can be easily applied to other diffusion-based SBDD\nframeworks.",
            "pdf_url": "http://arxiv.org/pdf/2406.16821v1",
            "published": "2024-06-24 17:31:41+00:00",
            "updated": "2024-06-24 17:31:41+00:00"
        },
        {
            "title": "Generative Fractional Diffusion Models",
            "authors": "Gabriel Nobis, Maximilian Springenberg, Marco Aversa, Michael Detzel, Rembert Daems, Roderick Murray-Smith, Shinichi Nakajima, Sebastian Lapuschkin, Stefano Ermon, Tolga Birdal, Manfred Opper, Christoph Knochenhauer, Luis Oala, Wojciech Samek",
            "summary": "We introduce the first continuous-time score-based generative model that\nleverages fractional diffusion processes for its underlying dynamics. Although\ndiffusion models have excelled at capturing data distributions, they still\nsuffer from various limitations such as slow convergence, mode-collapse on\nimbalanced data, and lack of diversity. These issues are partially linked to\nthe use of light-tailed Brownian motion (BM) with independent increments. In\nthis paper, we replace BM with an approximation of its non-Markovian\ncounterpart, fractional Brownian motion (fBM), characterized by correlated\nincrements and Hurst index $H \\in (0,1)$, where $H=1/2$ recovers the classical\nBM. To ensure tractable inference and learning, we employ a recently\npopularized Markov approximation of fBM (MA-fBM) and derive its reverse time\nmodel, resulting in generative fractional diffusion models (GFDMs). We\ncharacterize the forward dynamics using a continuous reparameterization trick\nand propose an augmented score matching loss to efficiently learn the\nscore-function, which is partly known in closed form, at minimal added cost.\nThe ability to drive our diffusion model via fBM provides flexibility and\ncontrol. $H \\leq 1/2$ enters the regime of rough paths whereas $H>1/2$\nregularizes diffusion paths and invokes long-term memory as well as a\nheavy-tailed behaviour (super-diffusion). The Markov approximation allows added\ncontrol by varying the number of Markov processes linearly combined to\napproximate fBM. Our evaluations on real image datasets demonstrate that GFDM\nachieves greater pixel-wise diversity and enhanced image quality, as indicated\nby a lower FID, offering a promising alternative to traditional diffusion\nmodels.",
            "pdf_url": "http://arxiv.org/pdf/2310.17638v2",
            "published": "2023-10-26 17:53:24+00:00",
            "updated": "2024-06-24 17:00:44+00:00"
        },
        {
            "title": "LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multi-modal Foundation Models",
            "authors": "Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao",
            "summary": "Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\nLatentExplainer, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\nLatentExplainer tackles three main challenges: inferring the meaning of latent\nvariables, aligning explanations with inductive biases, and handling varying\ndegrees of explainability. By perturbing latent variables and interpreting\nchanges in generated data, the framework provides a systematic approach to\nunderstanding and controlling the data generation process, enhancing the\ntransparency and interpretability of deep generative models. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations of\nlatent variables.",
            "pdf_url": "http://arxiv.org/pdf/2406.14862v2",
            "published": "2024-06-21 04:39:03+00:00",
            "updated": "2024-06-24 15:30:34+00:00"
        },
        {
            "title": "Repulsive Score Distillation for Diverse Sampling of Diffusion Models",
            "authors": "Nicolas Zilberstein, Morteza Mardani, Santiago Segarra",
            "summary": "Score distillation sampling has been pivotal for integrating diffusion models\ninto generation of complex visuals. Despite impressive results it suffers from\nmode collapse and lack of diversity. To cope with this challenge, we leverage\nthe gradient flow interpretation of score distillation to propose Repulsive\nScore Distillation (RSD). In particular, we propose a variational framework\nbased on repulsion of an ensemble of particles that promotes diversity. Using a\nvariational approximation that incorporates a coupling among particles, the\nrepulsion appears as a simple regularization that allows interaction of\nparticles based on their relative pairwise similarity, measured e.g., via\nradial basis kernels. We design RSD for both unconstrained and constrained\nsampling scenarios. For constrained sampling we focus on inverse problems in\nthe latent space that leads to an augmented variational formulation, that\nstrikes a good balance between compute, quality and diversity. Our extensive\nexperiments for text-to-image generation, and inverse problems demonstrate that\nRSD achieves a superior trade-off between diversity and quality compared with\nstate-of-the-art alternatives.",
            "pdf_url": "http://arxiv.org/pdf/2406.16683v1",
            "published": "2024-06-24 14:43:02+00:00",
            "updated": "2024-06-24 14:43:02+00:00"
        }
    ],
    "Quantitative Finance": []
}