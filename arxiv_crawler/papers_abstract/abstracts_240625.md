# Abstracts of Papers

## Physics
### GeoMFormer: A General Architecture for Geometric Molecular Representation Learning
**Authors**: Tianlang Chen, Shengjie Luo, Di He, Shuxin Zheng, Tie-Yan Liu, Liwei Wang

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16853v1](http://arxiv.org/pdf/2406.16853v1)

**Abstract**: Molecular modeling, a central topic in quantum mechanics, aims to accurately
calculate the properties and simulate the behaviors of molecular systems. The
molecular model is governed by physical laws, which impose geometric
constraints such as invariance and equivariance to coordinate rotation and
translation. While numerous deep learning approaches have been developed to
learn molecular representations under these constraints, most of them are built
upon heuristic and costly modules. We argue that there is a strong need for a
general and flexible framework for learning both invariant and equivariant
features. In this work, we introduce a novel Transformer-based molecular model
called GeoMFormer to achieve this goal. Using the standard Transformer modules,
two separate streams are developed to maintain and learn invariant and
equivariant representations. Carefully designed cross-attention modules bridge
the two streams, allowing information fusion and enhancing geometric modeling
in each stream. As a general and flexible architecture, we show that many
previous architectures can be viewed as special instantiations of GeoMFormer.
Extensive experiments are conducted to demonstrate the power of GeoMFormer. All
empirical results show that GeoMFormer achieves strong performance on both
invariant and equivariant tasks of different types and scales. Code and models
will be made publicly available at https://github.com/c-tl/GeoMFormer.


### Improving physics-informed DeepONets with hard constraints
**Authors**: Rüdiger Brecht, Dmytro R. Popovych, Alex Bihlo, Roman O. Popovych

**Published Date**: 2023-09-14

**Updated Date**: 2024-06-24

**PDF Url**: [2309.07899v2](http://arxiv.org/pdf/2309.07899v2)

**Abstract**: Current physics-informed (standard or deep operator) neural networks still
rely on accurately learning the initial and/or boundary conditions of the
system of differential equations they are solving. In contrast, standard
numerical methods involve such conditions in computations without needing to
learn them. In this study, we propose to improve current physics-informed deep
learning strategies such that initial and/or boundary conditions do not need to
be learned and are represented exactly in the predicted solution. Moreover,
this method guarantees that when a deep operator network is applied multiple
times to time-step a solution of an initial value problem, the resulting
function is at least continuous.


### Realizing a spatially correlated lattice interferometer
**Authors**: Peng Peng, Dekai Mao, Yi Liang, Guoling Yin, Hongmian Shui, Bo Song, Xiaoji Zhou

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16847v1](http://arxiv.org/pdf/2406.16847v1)

**Abstract**: Atom interferometers provide a powerful tool for measuring physical constants
and testifying fundamental physics with unprecedented precision. Conventional
atom interferometry focuses on the phase difference between two paths and
utilizes matter waves with fixed coherence. Here, we report on realizing a
Ramsey-Bord\'e interferometer of coherent matter waves dressed by a moving
optical lattice in the gravity direction, and explore the resulting
interference along multiple paths with tunable coherence. We investigate
spatial correlations of atoms both within the lattice and between two arms by
interferometry, and observe the emerging multiple interference peaks owing to
the long-range coherence nature of the Bose-Einstein condensate. Our findings
agree well with theoretical simulations, paving the way for high-precision
interferometry with ultracold atoms.


### Scaling and renormalization in high-dimensional regression
**Authors**: Alexander Atanasov, Jacob A. Zavatone-Veth, Cengiz Pehlevan

**Published Date**: 2024-05-01

**Updated Date**: 2024-06-24

**PDF Url**: [2405.00592v2](http://arxiv.org/pdf/2405.00592v2)

**Abstract**: This paper presents a succinct derivation of the training and generalization
performance of a variety of high-dimensional ridge regression models using the
basic tools of random matrix theory and free probability. We provide an
introduction and review of recent results on these topics, aimed at readers
with backgrounds in physics and deep learning. Analytic formulas for the
training and generalization errors are obtained in a few lines of algebra
directly from the properties of the $S$-transform of free probability. This
allows for a straightforward identification of the sources of power-law scaling
in model performance. We compute the generalization error of a broad class of
random feature models. We find that in all models, the $S$-transform
corresponds to the train-test generalization gap, and yields an analogue of the
generalized-cross-validation estimator. Using these techniques, we derive
fine-grained bias-variance decompositions for a very general class of random
feature models with structured covariates. These novel results allow us to
discover a scaling regime for random feature models where the variance due to
the features limits performance in the overparameterized setting. We also
demonstrate how anisotropic weight structure in random feature models can limit
performance and lead to nontrivial exponents for finite-width corrections in
the overparameterized setting. Our results extend and provide a unifying
perspective on earlier models of neural scaling laws.


### Damping effects of viscous dissipation on growth of symmetric instability
**Authors**: Laur Ferris, Donglai Gong

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16818v1](http://arxiv.org/pdf/2406.16818v1)

**Abstract**: Symmetric instability (SI) is a frontal instability arising from the
interaction of rotation with lateral and vertical shear of a frontal jet and is
a generalization of shear, centrifugal, and gravitational instabilities. While
the onset of SI has been studied in numerous observations and models, intuition
about its growth in physical ocean comes primarily from constant-viscosity
linear instability analysis and large eddy simulation (LES). A forward cascade
arising from SI in the real ocean, where numerous fine-to-microscale processes
interact with growing SI velocity cells, is less understood. While many
instances of symmetrically unstable flow have been observed, observations of
enhanced turbulent kinetic energy (TKE) dissipation ($\epsilon$) at these sites
are less common. We use numerical instability analysis of an idealized
geostrophic jet to show that viscous-diffusive effects of preexisting
turbulence from other turbulent processes (e.g., competing instabilities,
internal wave processes, or boundary layer processes) can suppress the growth
of SI in the real ocean. For example, a moderate level of ambient turbulence,
represented by uniform diffusivity and viscosity of $\kappa =\nu = 10^{-4}
m^2/s$, restricts the wavelength range of SI's fastest-growing mode from
$\mathcal{O}(10-100)$m to $\mathcal{O}(100)$m and elongates its e-folding
timescale by $\mathcal{O}(1-10)$ hrs; suggesting the net viscous-diffusive
effects of preexisting turbulence can damp the growth of SI. Viscous damping is
one possible explanation for the rarity of SI structures in the real ocean, and
our results motivate the inclusion of dependence on previous-timestep
$\epsilon$ or $\kappa$ when parameterizing SI in regional models.


### Discovering neutrino tridents at the Large Hadron Collider
**Authors**: Wolfgang Altmannshofer, Toni Mäkelä, Subir Sarkar, Sebastian Trojanowski, Keping Xie, Bei Zhou

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16803v1](http://arxiv.org/pdf/2406.16803v1)

**Abstract**: Neutrino trident production of di-lepton pairs is well recognized as a
sensitive probe of both electroweak physics and physics beyond the Standard
Model. Although a rare process, it could be significantly boosted by such new
physics, and it also allows the electroweak theory to be tested in a new
regime. We demonstrate that the forward neutrino physics program at the Large
Hadron Collider offers a promising opportunity to measure for the first time,
dimuon neutrino tridents with a statistical significance exceeding $5\sigma$.
We present predictions for various proposed experiments and outline a specific
experimental strategy to identify the signal and mitigate backgrounds, based on
"reverse tracking" dimuon pairs in the FASER$\nu$2 detector. We also discuss
prospects for constraining beyond Standard Model contributions to neutrino
trident rates at high energies.


### WISER: multimodal variational inference for full-waveform inversion without dimensionality reduction
**Authors**: Ziyi Yin, Rafael Orozco, Felix J. Herrmann

**Published Date**: 2024-05-03

**Updated Date**: 2024-06-24

**PDF Url**: [2405.10327v2](http://arxiv.org/pdf/2405.10327v2)

**Abstract**: We present a semi-amortized variational inference framework designed for
computationally feasible uncertainty quantification in 2D full-waveform
inversion to explore the multimodal posterior distribution without
dimensionality reduction. The framework is called WISER, short for
full-Waveform variational Inference via Subsurface Extensions with Refinements.
WISER leverages the power of generative artificial intelligence to perform
approximate amortized inference that is low-cost albeit showing an amortization
gap. This gap is closed through non-amortized refinements that make frugal use
of acoustic wave physics. Case studies illustrate that WISER is capable of
full-resolution, computationally feasible, and reliable uncertainty estimates
of velocity models and imaged reflectivities.


### A Mereological Approach to Higher-Order Structure in Complex Systems: from Macro to Micro with Möbius
**Authors**: Abel Jansma

**Published Date**: 2024-04-17

**Updated Date**: 2024-06-24

**PDF Url**: [2404.14423v3](http://arxiv.org/pdf/2404.14423v3)

**Abstract**: Relating microscopic interactions to macroscopic observables is a central
challenge in the study of complex systems. Addressing this question requires
understanding both pairwise and \textit{higher-order} interactions, but the
latter are less well understood. Here, we show that the M\"obius inversion
theorem provides a general mathematical formalism for deriving higher-order
interactions from macroscopic observables, relative to a chosen decomposition
of the system into parts. Applying this framework to a diverse range of
systems, we demonstrate that many existing notions of higher-order
interactions, from epistasis in genetics and many-body couplings in physics, to
synergy in game theory and artificial intelligence, naturally and uniquely
arise from an appropriate mereological decomposition. By revealing the common
mathematical structure underlying seemingly disparate phenomena, our work
highlights the fundamental role of decomposition choice in the definition and
estimation of higher-order interactions. We discuss how this unifying
perspective can facilitate the transfer of insights between domains, guide the
selection of appropriate system decompositions, and motivate the search for
novel interaction types through new decomposition strategies. To illustrate how
this works in practice, we derive a new decomposition of the KL-divergence, and
show that it correctly disentangles divergences at different orders on
simulated spin models. Our results suggest that the M\"obius inversion theorem
provides a powerful and practical lens for understanding the emergence of
complex behaviour from the interplay of microscopic parts, with applications
across a wide range of disciplines.


### Quantum resolution limit of long-baseline imaging using distributed entanglement
**Authors**: Isack Padilla, Aqil Sajjad, Babak N. Saif, Saikat Guha

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16789v1](http://arxiv.org/pdf/2406.16789v1)

**Abstract**: It has been shown that shared entanglement between two telescope sites can in
principle be used to localize a point source by mimicking the standard
phase-scanning interferometer, but without physically bringing the light from
the distant telescopes together. In this paper, we show that a receiver that
employs spatial-mode sorting at each telescope site, combined with pre-shared
entanglement and local quantum operations can be used to mimic the most general
multimode interferometer acting on light collected from the telescopes. As an
example application to a quantitative passive-imaging problem, we show that the
quantum-limited precision of estimating the angular separation between two
stars can be attained by an instantiation of the aforesaid entanglement based
receiver. We discuss how this entanglement assisted strategy can be used to
achieve the quantum-limited precision of any complex quantitative imaging task
involving any number of telescopes. We provide a blueprint of this general
receiver that involves quantum transduction of starlight into quantum memory
banks and spatial mode sorters deployed at each telescope site, and
measurements that include optical detection as well as qubit gates and
measurements on the quantum memories. We discuss the relative contributions of
local mode sorting at telescope sites vis-a-vis distributed
entanglement-assisted interferometry, to the overall quantum-limited
information about the scene, based on the ratio of the baseline distance to the
individual telescope diameter.


### OlympicArena Medal Ranks: Who Is the Most Intelligent AI So Far?
**Authors**: Zhen Huang, Zengzhi Wang, Shijie Xia, Pengfei Liu

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16772v1](http://arxiv.org/pdf/2406.16772v1)

**Abstract**: In this report, we pose the following question: Who is the most intelligent
AI model to date, as measured by the OlympicArena (an Olympic-level,
multi-discipline, multi-modal benchmark for superintelligent AI)? We
specifically focus on the most recently released models: Claude-3.5-Sonnet,
Gemini-1.5-Pro, and GPT-4o. For the first time, we propose using an Olympic
medal Table approach to rank AI models based on their comprehensive performance
across various disciplines. Empirical results reveal: (1) Claude-3.5-Sonnet
shows highly competitive overall performance over GPT-4o, even surpassing
GPT-4o on a few subjects (i.e., Physics, Chemistry, and Biology). (2)
Gemini-1.5-Pro and GPT-4V are ranked consecutively just behind GPT-4o and
Claude-3.5-Sonnet, but with a clear performance gap between them. (3) The
performance of AI models from the open-source community significantly lags
behind these proprietary models. (4) The performance of these models on this
benchmark has been less than satisfactory, indicating that we still have a long
way to go before achieving superintelligence. We remain committed to
continuously tracking and evaluating the performance of the latest powerful
models on this benchmark (available at
https://github.com/GAIR-NLP/OlympicArena).


## Diffusion
### StableNormal: Reducing Diffusion Variance for Stable and Sharp Normal
**Authors**: Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, Xiaoguang Han

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16864v1](http://arxiv.org/pdf/2406.16864v1)

**Abstract**: This work addresses the challenge of high-quality surface normal estimation
from monocular colored inputs (i.e., images and videos), a field which has
recently been revolutionized by repurposing diffusion priors. However, previous
attempts still struggle with stochastic inference, conflicting with the
deterministic nature of the Image2Normal task, and costly ensembling step,
which slows down the estimation process. Our method, StableNormal, mitigates
the stochasticity of the diffusion process by reducing inference variance, thus
producing "Stable-and-Sharp" normal estimates without any additional ensembling
process. StableNormal works robustly under challenging imaging conditions, such
as extreme lighting, blurring, and low quality. It is also robust against
transparent and reflective surfaces, as well as cluttered scenes with numerous
objects. Specifically, StableNormal employs a coarse-to-fine strategy, which
starts with a one-step normal estimator (YOSO) to derive an initial normal
guess, that is relatively coarse but reliable, then followed by a
semantic-guided refinement process (SG-DRN) that refines the normals to recover
geometric details. The effectiveness of StableNormal is demonstrated through
competitive performance in standard datasets such as DIODE-indoor, iBims,
ScannetV2 and NYUv2, and also in various downstream tasks, such as surface
reconstruction and normal enhancement. These results evidence that StableNormal
retains both the "stability" and "sharpness" for accurate normal estimation.
StableNormal represents a baby attempt to repurpose diffusion priors for
deterministic estimation. To democratize this, code and models have been
publicly available in hf.co/Stable-X


### General Binding Affinity Guidance for Diffusion Models in Structure-Based Drug Design
**Authors**: Yue Jian, Curtis Wu, Danny Reidenbach, Aditi S. Krishnapriyan

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16821v1](http://arxiv.org/pdf/2406.16821v1)

**Abstract**: Structure-Based Drug Design (SBDD) focuses on generating valid ligands that
strongly and specifically bind to a designated protein pocket. Several methods
use machine learning for SBDD to generate these ligands in 3D space,
conditioned on the structure of a desired protein pocket. Recently, diffusion
models have shown success here by modeling the underlying distributions of
atomic positions and types. While these methods are effective in considering
the structural details of the protein pocket, they often fail to explicitly
consider the binding affinity. Binding affinity characterizes how tightly the
ligand binds to the protein pocket, and is measured by the change in free
energy associated with the binding process. It is one of the most crucial
metrics for benchmarking the effectiveness of the interaction between a ligand
and protein pocket. To address this, we propose BADGER: Binding Affinity
Diffusion Guidance with Enhanced Refinement. BADGER is a general guidance
method to steer the diffusion sampling process towards improved protein-ligand
binding, allowing us to adjust the distribution of the binding affinity between
ligands and proteins. Our method is enabled by using a neural network (NN) to
model the energy function, which is commonly approximated by AutoDock Vina
(ADV). ADV's energy function is non-differentiable, and estimates the affinity
based on the interactions between a ligand and target protein receptor. By
using a NN as a differentiable energy function proxy, we utilize the gradient
of our learned energy function as a guidance method on top of any trained
diffusion model. We show that our method improves the binding affinity of
generated ligands to their protein receptors by up to 60\%, significantly
surpassing previous machine learning methods. We also show that our guidance
method is flexible and can be easily applied to other diffusion-based SBDD
frameworks.


### Generative Fractional Diffusion Models
**Authors**: Gabriel Nobis, Maximilian Springenberg, Marco Aversa, Michael Detzel, Rembert Daems, Roderick Murray-Smith, Shinichi Nakajima, Sebastian Lapuschkin, Stefano Ermon, Tolga Birdal, Manfred Opper, Christoph Knochenhauer, Luis Oala, Wojciech Samek

**Published Date**: 2023-10-26

**Updated Date**: 2024-06-24

**PDF Url**: [2310.17638v2](http://arxiv.org/pdf/2310.17638v2)

**Abstract**: We introduce the first continuous-time score-based generative model that
leverages fractional diffusion processes for its underlying dynamics. Although
diffusion models have excelled at capturing data distributions, they still
suffer from various limitations such as slow convergence, mode-collapse on
imbalanced data, and lack of diversity. These issues are partially linked to
the use of light-tailed Brownian motion (BM) with independent increments. In
this paper, we replace BM with an approximation of its non-Markovian
counterpart, fractional Brownian motion (fBM), characterized by correlated
increments and Hurst index $H \in (0,1)$, where $H=1/2$ recovers the classical
BM. To ensure tractable inference and learning, we employ a recently
popularized Markov approximation of fBM (MA-fBM) and derive its reverse time
model, resulting in generative fractional diffusion models (GFDMs). We
characterize the forward dynamics using a continuous reparameterization trick
and propose an augmented score matching loss to efficiently learn the
score-function, which is partly known in closed form, at minimal added cost.
The ability to drive our diffusion model via fBM provides flexibility and
control. $H \leq 1/2$ enters the regime of rough paths whereas $H>1/2$
regularizes diffusion paths and invokes long-term memory as well as a
heavy-tailed behaviour (super-diffusion). The Markov approximation allows added
control by varying the number of Markov processes linearly combined to
approximate fBM. Our evaluations on real image datasets demonstrate that GFDM
achieves greater pixel-wise diversity and enhanced image quality, as indicated
by a lower FID, offering a promising alternative to traditional diffusion
models.


### LatentExplainer: Explaining Latent Representations in Deep Generative Models with Multi-modal Foundation Models
**Authors**: Mengdan Zhu, Raasikh Kanjiani, Jiahui Lu, Andrew Choi, Qirui Ye, Liang Zhao

**Published Date**: 2024-06-21

**Updated Date**: 2024-06-24

**PDF Url**: [2406.14862v2](http://arxiv.org/pdf/2406.14862v2)

**Abstract**: Deep generative models like VAEs and diffusion models have advanced various
generation tasks by leveraging latent variables to learn data distributions and
generate high-quality samples. Despite the field of explainable AI making
strides in interpreting machine learning models, understanding latent variables
in generative models remains challenging. This paper introduces
LatentExplainer, a framework for automatically generating semantically
meaningful explanations of latent variables in deep generative models.
LatentExplainer tackles three main challenges: inferring the meaning of latent
variables, aligning explanations with inductive biases, and handling varying
degrees of explainability. By perturbing latent variables and interpreting
changes in generated data, the framework provides a systematic approach to
understanding and controlling the data generation process, enhancing the
transparency and interpretability of deep generative models. We evaluate our
proposed method on several real-world and synthetic datasets, and the results
demonstrate superior performance in generating high-quality explanations of
latent variables.


### Repulsive Score Distillation for Diverse Sampling of Diffusion Models
**Authors**: Nicolas Zilberstein, Morteza Mardani, Santiago Segarra

**Published Date**: 2024-06-24

**Updated Date**: 2024-06-24

**PDF Url**: [2406.16683v1](http://arxiv.org/pdf/2406.16683v1)

**Abstract**: Score distillation sampling has been pivotal for integrating diffusion models
into generation of complex visuals. Despite impressive results it suffers from
mode collapse and lack of diversity. To cope with this challenge, we leverage
the gradient flow interpretation of score distillation to propose Repulsive
Score Distillation (RSD). In particular, we propose a variational framework
based on repulsion of an ensemble of particles that promotes diversity. Using a
variational approximation that incorporates a coupling among particles, the
repulsion appears as a simple regularization that allows interaction of
particles based on their relative pairwise similarity, measured e.g., via
radial basis kernels. We design RSD for both unconstrained and constrained
sampling scenarios. For constrained sampling we focus on inverse problems in
the latent space that leads to an augmented variational formulation, that
strikes a good balance between compute, quality and diversity. Our extensive
experiments for text-to-image generation, and inverse problems demonstrate that
RSD achieves a superior trade-off between diversity and quality compared with
state-of-the-art alternatives.


## Quantitative Finance
